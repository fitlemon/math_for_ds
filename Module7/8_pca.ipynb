{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель занятия — изучить метод главных компонент (PCA) для решения задач снижения размерности на подготовленных и неподготовленных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Метод главных компонент (Principal Component Analysis, PCA)</b> — это статистический метод, используемый для уменьшения размерности данных и извлечения наиболее значимых информационных характеристик из множества переменных. Он позволяет сжать исходный набор данных, сохраняя при этом наибольшее количество информации.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод основан на линейном преобразовании данных с целью найти новые оси (главные компоненты), вдоль которых данные имеют наибольшую изменчивость. Главные компоненты — это линейные комбинации исходных признаков, они ортогональны (перпендикулярны) друг другу и упорядочены по убыванию объяснённой ими дисперсии данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять принципы работы PCA, необходимо рассмотреть некоторые понятия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Дисперсия</b> — это мера разброса или изменчивости случайной величины. Она показывает, насколько сильно значения случайной величины отклоняются от её среднего значения.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случайной величины $ X $ с функцией вероятности (или функцией плотности вероятности) $ f(x) $ и средним значением $ \\mu$, дисперсия $ \\sigma^2 $ определяется следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma^2=E\\left[(X-\\mu)^2\\right]=\\int(x-\\mu)^2 f(x) d x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $E $ обозначает математическое ожидание, а интеграл берётся по всем возможным значениям $x $ случайной величины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дисперсия является положительным числом и представляет собой среднюю квадратическую разницу между значениями случайной величины и её средним значением. Чем больше дисперсия, тем больше вариативность или разброс значений случайной величины.\n",
    "\n",
    "В контексте PCA объяснённая дисперсия относится к количеству дисперсии в исходных данных, которая объясняется каждой главной компонентой.\n",
    "\n",
    "Среднее значение первой главной компоненты вычисляется суммированием всех значений первой главной компоненты и делением этой суммы на общее количество наблюдений.\n",
    "\n",
    "Математически это можно выразить следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text { Среднее }_{P C 1}=\\frac{\\sum_{i=1}^n P C 1_i}{n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где:\n",
    "\n",
    "$ PC1_{i} $ —  значения первой главной компоненты для каждого наблюдения; <br>\n",
    "$ n $ — общее количество наблюдений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснённую дисперсию первой главной компоненты нужно записать как:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично можно вычислить объяснённую дисперсию для других главных компонент.\n",
    "\n",
    "Объяснённая дисперсия позволяет определить, какую долю дисперсии данных объясняет каждая главная компонента. Чем больше объяснённая дисперсия, тем больше информации оригинальных данных сохраняется в соответствующей главной компоненте.\n",
    "\n",
    "PCA находит главные компоненты путём вычисления собственных значений и собственных векторов ковариационной матрицы данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Ковариация</b> — это мера статистической зависимости между двумя случайными переменными. Она измеряет, насколько взаимосвязаны изменения двух переменных.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ковариация может быть:\n",
    "\n",
    "-    положительной, если две переменные сильно положительно коррелируют (увеличение одной переменной связано с увеличением другой);\n",
    "-    отрицательной, если они сильно отрицательно коррелируют (увеличение одной переменной связано с уменьшением другой);\n",
    "-    близкой к нулю, если между ними нет линейной зависимости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_19.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Положительная, отрицательная и нулевая ковариация переменных X и Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ковариацию двух случайных переменных и можно записать следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\operatorname{cov}(X, Y)=E[(X-E[X])(Y-E[Y])] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $ E[X] $ и $ E[Y] $ обозначают математические ожидания (средние значения) переменных $X $  и $ Y $ соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_20.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, порядок указания переменных в ковариации не имеет значения, и результат будет одинаковым независимо от того, какую пару переменных мы рассматриваем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Собственные векторы матрицы</b> — это векторы, которые при умножении на эту матрицу остаются коллинеарными (линейно зависимыми) сами себе, изменяясь только в масштабе.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представление о собственных векторах и значениях представлено на рисунке ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_21.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально собственные векторы определяются как ненулевые векторы , для которых выполняется следующее уравнение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_22.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, умножение матрицы $ A $ на собственный вектор $v$ приводит к получению нового вектора, который параллелен $v $ и масштабирован в $ \\lambda$ раз. Собственные векторы матрицы играют важную роль в линейной алгебре. Они позволяют анализировать свойства и поведение матрицы: например, определять направления наибольшей и наименьшей изменчивости (главные компоненты) или решать системы линейных дифференциальных уравнений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">   <b>Собственные значения матрицы</b> — это значения, которые удовлетворяют уравнению $ A *v = \\lambda *v $, где $ A $ — исходная матрица, $v$ — собственное значение (скаляр), а $ \\lambda $ — собственный вектор.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иными словами, собственные значения матрицы являются корнями уравнения $ det(A - \\lambda I $), где $ I $ — единичная матрица, а $ det $ обозначает определитель матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_23.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственный вектор, обозначенный синим цветом, остаётся параллельным самому себе при деформации или преобразовании. Это означает, что его направление не меняется, а только масштабируется (изменяется длина).\n",
    "\n",
    "Такой вектор является собственным вектором преобразования, соответствующим определённому собственному значению $ \\lambda $ (в данном случае $ \\lambda = 1 $), так как его направление сохраняется. Важно отметить, что любой вектор, параллельный синему собственному вектору, также будет собственным вектором с собственным значением $ \\lambda $.\n",
    "\n",
    "Синий вектор меняет направление при деформации или преобразовании, в то время как красный вектор сохраняет своё направление. Поэтому красный вектор является собственным вектором, а синий — нет. Красный вектор не растягивается и не сжимается, его длина остаётся неизменной, поэтому соответствующее собственное значение равно единице, как показано на рисунке ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_24.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственные значения и собственные векторы играют важную роль в различных областях, таких как линейная алгебра, теория графов, физика, машинное обучение и другие. Они позволяют анализировать свойства и поведение матрицы, а также решать различные задачи, связанные с линейными операциями над данными.\n",
    "\n",
    "Метод главных компонент и сингулярное разложение матриц (Singular Value Decomposition, SVD) — это два связанных понятия в анализе данных и линейной алгебре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_25.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуальная демонстрация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объекты в наборах данных можно представить разным количеством признаков. Мы можем визуализировать только те наборы данных, в которых не более трёх признаков.\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_26.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм PCA позволяет снизить количество признаков для визуализации или уменьшения размера датасета. В общем случае количество признаков можно снизить весьма существенно. Например, для визуализации набора данных MNIST происходит снижение размерности с 784 до 2. Однако в учебных целях мы будем преобразовывать данные в двумерном пространстве, произведя центрирование данных и смену осей.\n",
    "\n",
    "Визуально алгоритм PCA можно представить в виде набора следующих шагов.\n",
    "\n",
    "1.   Сначала мы читаем текущие данные и находим средние значения по каждому признаку, получая таким образом новый центр координат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_27.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Затем перемещаем точку начала координат в этот новый усреднённый центр нашего набора данных. Взаимное расположение объектов при этом сохраняется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_28.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. После находим линию (на рисунке ниже обозначена красным пунктиром), проходящую через новый центр координат, сумма расстояний от точек данных до которой минимальна. Для наглядности используется итеративный алгоритм. На практике применяют другие техники, например ковариационную матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_29.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение линии, сумма расстояний от точек данных до которой минимальна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. После этого, зная коэффициент наклона найденной прямой, находим линейную комбинацию исходных признаков, которая определяет вектор, сонаправленный найденной линии, сумма расстояний от точек данных до которой минимальна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормировав все элементы линейной комбинации на длину этого вектора, мы получим собственный вектор первой главной компоненты длины 1 (на рисунке ниже — синего цвета).\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_30.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахождение линии, сумма расстояний от точек данных до которой минимальна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно найти собственные значения главной компоненты. Каждое собственное значение соответствует одной главной компоненте и показывает, какую часть дисперсии в исходных данных объясняет соответствующая компонента. Сумма всех собственных значений равна общей дисперсии исходных данных.\n",
    "\n",
    "Собственные значения используют для выбора количества главных компонент, которые нужно оставить после снижения размерности с помощью PCA. Чем больше собственное значение, тем больше доля дисперсии объясняется соответствующей главной компонентой. Обычно выбирают те главные компоненты, которые объясняют большую часть общей дисперсии (например, 95 или 99 %), и игнорируют остальные компоненты, чтобы сократить размерность данных и сохранить наиболее информативные характеристики.\n",
    "\n",
    "В наглядном примере формула для вычисления собственного значения главной компоненты равна:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\lambda=\\frac{\\sum_{i=1}^n \\text { расстояние }{ }^2 \\text { от } i-\\text { го примера до линии главной компоненты }}{n-1} $ \n",
    "где $n$ — количество объектов в наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике формула для вычисления собственных значений в методе главных компонент (PCA) основана на ковариационной матрице данных.\n",
    "\n",
    "Пусть $C $ — ковариационная матрица размерности $d$×$d$ для исходных данных, где $d$— количество признаков.\n",
    "\n",
    "Собственные значения $ \\lambda $ можно получить как корни характеристического уравнения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ det(C - \\lambda I)=0 $ где $I$ — единичная матрица размерности $d$×$d$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решив это уравнение, можно получить собственные значения $\\lambda _1 $, $\\lambda _2 $ ... , $\\lambda _d $\n",
    "\n",
    ".\n",
    "\n",
    "Обратите внимание, что собственные значения обычно упорядочивают по убыванию, что позволяет выбрать наиболее информативные главные компоненты, которые объясняют наибольшую часть дисперсии в данных.\n",
    "\n",
    "Вторая главная компонента будет перпендикулярна первой. Аналогично, зная угловой коэффициент линии, перпендикулярной первой главной компоненте, мы можем найти линейные комбинации признаков, которые формируют вектор второй главной компоненты.\n",
    "\n",
    "Отнормировав все элементы линейной комбинации на длину полученного вектора, мы получим собственный вектор второй главной компоненты.\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_31.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если есть необходимость получить дальнейшие компоненты, процесс их поиска будет идти аналогичным образом.\n",
    "\n",
    "В нашем случае мы остановимся после нахождения первых двух главных компонент. После вычисления этих компонент необходимо повернуть график таким образом, чтобы первая главная компонента стала новой осью абсцисс (осью X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_32.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, главные компоненты 1 и 2 становятся новыми осями координат.\n",
    "\n",
    "Необходимо помнить, что каждая следующая главная компонента будет ортогональной предыдущим главным компонентам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_33.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также необходимо помнить, что чем выше собственные значения первых главных компонент, тем лучше они сохраняют информацию об исходном датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_34.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слева — высокая доля объяснённой дисперсии среди первых главных компонент (график слева). Справа — относительно низкая доля объяснённой дисперсии среди первых главных компонент.\n",
    "\n",
    "Если доля объяснённой дисперсии среди первых компонент недостаточно высока, это может привести к снижению качества признакового описания, полученного в результате работы PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных для алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлен общий процесс подготовки данных для использования с методом главных компонент (PCA):\n",
    "\n",
    "1.    Масштабирование данных. Рекомендуется масштабировать признаки входных данных перед применением PCA, особенно если признаки имеют различные шкалы или единицы измерения. Для приведения к общей шкале обычно используют стандартизацию или нормализацию данных.\n",
    "2.    Обработка категориальных переменных. Если в данных есть категориальные переменные, требуется преобразовать их в числовой формат. Для этого можно использовать методы One-Hot Encoding или Label Encoding.\n",
    "3.    Обработка пропущенных значений. Если в данных есть пропущенные значения, в зависимости от контекста данных нужно либо удалить строки или столбцы с пропущенными значениями, либо заполнить их средними или медианными значениями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_35.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценить качество алгоритма PCA (метода главных компонент) можно с помощью нескольких метрик и методов. Перечислим основные.\n",
    "\n",
    "Объяснённая дисперсия позволяет определить, какую часть общей дисперсии данных объясняют выбранные главные компоненты. Эта метрика даёт возможность оценить, насколько успешно PCA сжимает информацию о данных.\n",
    "\n",
    "Кумулятивная объяснённая дисперсия показывает, сколько общей дисперсии данных суммарно объяснено выбранными главными компонентами. Она может пригодиться, чтобы определить оптимальное число главных компонент для использования.\n",
    "\n",
    "Восстановление данных. Оценить качество алгоритма PCA можно путём восстановления данных из пространства главных компонент и сравнения восстановленных данных с исходными. Чтобы оценить точность восстановления данных, можно использовать различные метрики, такие как среднеквадратическая ошибка (MSE) или средняя абсолютная ошибка (MAE).\n",
    "\n",
    "Визуализация. Визуальное представление главных компонент и их влияния на данные может помочь в оценке качества алгоритма PCA. Построение графиков, например графика объяснённой дисперсии по числу главных компонент или графика сжатых данных в пространстве главных компонент, может помочь в понимании эффективности PCA.\n",
    "\n",
    "Задача. Наконец, оценить качество алгоритма PCA можно с учётом конкретной задачи или цели. Например, в задаче классификации можно оценить, как PCA влияет на точность классификации модели или какие признаки являются наиболее информативными после применения PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У алгоритма PCA широкий спектр применений. Некоторые примеры использования алгоритма:\n",
    "\n",
    "-    Снижение размерности данных. Это позволяет представить многомерные данные в более компактной форме, удалив менее информативные признаки и выделив наиболее важные компоненты. Снижение размерности может быть полезно для увеличения эффективности вычислений, улучшения визуализации данных и борьбы с проклятием размерности.\n",
    "-    Визуализация данных. PCA можно использовать для визуализации данных в двух или трёх измерениях. Применение PCA к исходным данным позволяет преобразовать их в новое пространство с меньшей размерностью, где можно визуализировать данные на плоскости или в пространстве. Это помогает выявить структуру и зависимости между объектами данных.\n",
    "-    Удаление корреляций. PCA можно использовать для удаления или уменьшения корреляции между признаками в данных. Это может быть полезно в случаях, когда сильная корреляция между признаками мешает моделированию или приводит к мультиколлинеарности.\n",
    "-    Анализ главных компонент. PCA позволяет выделить главные компоненты, которые объясняют наибольшую долю дисперсии в данных. Это помогает идентифицировать наиболее важные факторы или признаки, влияющие на данные, и понять их вклад в общую вариацию.\n",
    "-    Сжатие данных. PCA можно использовать для сжатия данных. При этом удаётся сохранить важную информацию и уменьшить объём памяти, необходимый для хранения данных. Это может быть полезно в задачах с ограниченными ресурсами или при передаче данных по сети.\n",
    "-    Предобработка данных. PCA можно использовать для предварительной обработки данных перед применением других алгоритмов машинного обучения. Он помогает улучшить производительность моделей и избежать проблемы мультиколлинеарности.\n",
    "\n",
    "Области применения PCA включают финансы, экономику, биоинформатику, обработку изображений, распознавание образов, геологию, маркетинг и многие другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плюсы и минусы алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плюсы\n",
    "\n",
    "-    Снижение размерности. PCA позволяет представить многомерные данные в пространстве меньшей размерности, сохраняя при этом наибольшую долю информации. Это увеличивает эффективность вычислений, уменьшает сложность моделей и улучшает визуализацию данных.\n",
    "-    Выделение наиболее информативных признаков. PCA позволяет определить наиболее важные компоненты данных, которые объясняют наибольшую долю вариации. Это помогает выделить наиболее информативные признаки и факторы, которые влияют на данные. Таким образом, PCA можно использовать для отбора признаков и уменьшения размерности без потери существенной информации.\n",
    "-    Устранение корреляций. PCA можно использовать для устранения или уменьшения корреляций между признаками. Это особенно полезно, когда сильная корреляция мешает моделированию или приводит к проблемам мультиколлинеарности.\n",
    "-    Визуализация данных. Применение PCA позволяет визуализировать данные в пространстве с меньшей размерностью. Это помогает визуально исследовать структуру данных, выявить паттерны и зависимости, а также обнаружить аномалии или выбросы.\n",
    "-    Сжатие данных. PCA можно использовать для сжатия данных. При этом уменьшается объём памяти, необходимый для хранения данных, и сохраняется основная структура и вариация данных. Сжатие данных может быть особенно полезным в задачах с ограниченными ресурсами или при передаче данных по сети.\n",
    "-    Предобработка данных. PCA можно использовать в качестве предварительной обработки данных перед применением других алгоритмов машинного обучения. Он помогает улучшить производительность моделей, уменьшить влияние шума и избежать проблемы мультиколлинеарности.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минусы\n",
    "\n",
    "-    Потеря интерпретируемости. При снижении размерности данных с помощью PCA исходные признаки объединяются в новые компоненты, которые являются линейными комбинациями исходных признаков. Это может привести к потере интерпретируемости, поскольку новые компоненты не всегда имеют прямую связь с исходными признаками.\n",
    "-    Зависимость от линейности. PCA предполагает линейные зависимости между признаками. Если данные имеют сложные нелинейные зависимости, PCA может быть неэффективным и не удастся захватить важную информацию, содержащуюся в данных.\n",
    "-    Чувствительность к выбросам. Алгоритм PCA может быть чувствительным к выбросам в данных. Выбросы могут сильно влиять на главные компоненты и искажать результаты анализа.\n",
    "-    Расчёт вычислительно сложен. Вычисление собственных значений и собственных векторов для больших наборов данных может быть вычислительно сложной задачей. Это особенно актуально, когда количество признаков или объектов велико.\n",
    "-    Не учитывает контекст и доменные знания. PCA основывается на математических принципах и не учитывает контекст и доменные знания данных. Это может ограничивать его способность улавливать специфические характеристики и зависимости в данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) в библиотеке scikit-learn реализован через <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">класс PCA</a> из модуля sklearn.decomposition.\n",
    "\n",
    "Основные параметры PCA:\n",
    "\n",
    "    n_components — количество главных компонент, которые требуется извлечь из данных. Обязательный параметр.\n",
    "    svd_solver — метод, используемый для вычисления PCA. Может принимать значения auto, full, arpack или randomized. По умолчанию используется auto, который выбирает наиболее подходящий метод автоматически.\n",
    "\n",
    "Класс PCA также имеет методы fit(X) — для обучения модели на данных X, transform(X) — для преобразования данных X с использованием изученных компонент, fit_transform(X) — для комбинированного обучения и преобразования данных.\n",
    "\n",
    "Давайте резюмируем.\n",
    "\n",
    "PCA (Principal Component Analysis) — это метод анализа данных, используемый для уменьшения размерности исходного набора данных. Основная идея PCA заключается в поиске новых независимых переменных, называемых главными компонентами, которые представляют наибольшую долю вариации в данных.\n",
    "\n",
    "Основные преимущества PCA:\n",
    "\n",
    "    Уменьшение размерности данных, что упрощает анализ и визуализацию.\n",
    "    Сокращение шума и выбросов в данных.\n",
    "    Сохранение наибольшей доли информации при уменьшении размерности данных.\n",
    "\n",
    "PCA — это мощный инструмент для анализа данных. Он широко применяется в машинном обучении, статистике, визуализации данных и в других областях, где требуется снижение размерности данных и выделение наиболее информативных признаков.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
