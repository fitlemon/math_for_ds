{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель занятия — познакомиться с обучением без учителя, кластеризацией и снижением размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое обучение без учителя?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Обучение без учителя (unsupervised learning)</b> — это подраздел машинного обучения, идея которого состоит в извлечении информации и паттернов из неструктурированных (неразмеченных) данных.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от обучения с учителем, где модели предоставляются размеченные данные с правильными ответами, при обучении без учителя данные не имеют меток или правильных ответов. Основная цель обучения без учителя — найти скрытую структуру в данных или сгенерировать полезные представления данных без явной разметки.\n",
    "\n",
    "В результате обучения без учителя модель выявляет внутренние закономерности, группирует или кластеризует данные, находит скрытые факторы или создаёт низкоразмерные представления данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры алгоритмов обучения без учителя:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Методы кластеризации — k-средних, DBSCAN.\n",
    "- Методы снижения размерности — PCA, t-SNE.\n",
    "- Алгоритмы генеративных моделей — автокодировщики, генеративные состязательные сети и многие другие.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение без учителя широко применяется при анализе данных, обнаружении аномалий, сжатии данных, генерации контента и во многих других задачах, где необходимо извлекать полезную информацию из неразмеченных данных.\n",
    "\n",
    "Например, на рисунке ниже показано совместное применение алгоритмов PCA (снижение размерности) и k-средних (кластеризация) для визуализации датасета MNIST. В итоге алгоритм находит центроиды* кластеров и закрашивает определённым цветом области, наиболее близкие к каждому из центроидов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\"><b>*Центроиды</b> — это точки, представляющие центры каждого кластера. Они являются средними значениями точек внутри каждого кластера.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поговорим о двух классических подразделах обучения без учителя — кластеризации и снижении размерности данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  Кластеризация в машинном обучении относится к задачам обучения без учителя и заключается в группировке объектов данных в кластеры на основе их схожести или близости друг к другу.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы кластеризации стремятся найти внутреннюю структуру в данных, основываясь на их признаках или свойствах, без заранее известного количества кластеров или информации о принадлежности объектов к определённым кластерам.\n",
    "\n",
    "Например, на рисунке ниже алгоритм Agglomerative Clustering выделяет монеты на фотографии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель кластеризации — максимизация схожести объектов внутри кластеров и минимизация схожести между кластерами.\n",
    "\n",
    "На следующем рисунке показан пример сжатия данных с помощью алгоритма k-means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распространённые варианты применения кластеризации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - <b>Поиск скрытых структур</b>. Кластерный анализ помогает выявить скрытые структуры и группы в данных. Это может быть полезно для идентификации паттернов, тенденций или сегментов, которые можно использовать для принятия более взвешенных решений.\n",
    "- <b>Сжатие данных</b>. Группируя похожие объекты в кластеры, можно сократить объём данных, сохраняя при этом их основные характеристики и структуру. Это может упростить дальнейший анализ или обработку данных.\n",
    "- <b>Рекомендательные системы</b>. С помощью кластеризации можно создавать группы пользователей или товаров со схожими характеристиками. Это может быть основой для разработки рекомендаций, чтобы предлагать пользователям похожие товары или связывать их с другими пользователями с такими же интересами.\n",
    "- <b>Сегментация аудитории</b>. Кластерный анализ может помочь в разделении аудитории на группы с общими характеристиками. Это может быть полезно для адаптации маркетинговых стратегий, персонализации коммуникации или создания целевых сообщений для каждой группы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеры — это группы объектов, которые обладают схожими характеристиками или свойствами, в то время как объекты из разных кластеров сильно отличаются друг от друга. Для измерения расстояний вводится понятие «центроид».\n",
    "\n",
    "Внутрикластерное расстояние и межкластерное расстояние — это две основные характеристики, которые учитываются при оценке качества кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Внутрикластерное расстояние (интракластерное расстояние)</b> — это мера сходства между объектами внутри одного кластера. Чем меньше среднее внутрикластерное расстояние, тем более компактными и однородными являются кластеры.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта формула вычисляет сумму всех парных расстояний между объектами внутри каждого кластера и затем усредняет это значение по всем кластерам. Низкое значение внутрикластерного расстояния указывает на то, что объекты внутри кластера находятся близко друг к другу, что желательно для качественной кластеризации.\n",
    "\n",
    "Определение низкого внутрикластерного расстояния зависит от контекста и специфики данных: нет универсального значения, которое можно было бы считать низким для всех случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\"><b>Межкластерное расстояние</b> — это мера различия между кластерами, которая определяет, насколько различные кластеры удалены друг от друга.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше межкластерное расстояние, тем более разделены и отделены друг от друга кластеры. Высокое межкластерное расстояние указывает на хорошую разделимость и различимость кластеров.\n",
    "\n",
    "Среднее межкластерное расстояние можно представить следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула вычисляет сумму всех парных расстояний между центроидами кластеров и затем усредняет это значение по всем парам кластеров.\n",
    "\n",
    "Далее рассмотрим несколько основных метрик качества кластеризации, построенных с помощью учёта расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коэффициент силуэта (Silhouette Coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент силуэта принимает значения от -1 до 1, где значение ближе к 1 указывает на хорошую кластеризацию, а ближе к -1 — на неправильную кластеризацию.\n",
    "\n",
    "Формула называется коэффициентом силуэта (Silhouette Coefficient), потому что основана на понятии «силуэта» как визуального представления объекта или кластера.\n",
    "\n",
    "Силуэт представляет собой меру компактности и разделённости объекта в контексте кластеризации. Визуально силуэт объекта можно представить как его очертание, отражающее степень близости объекта к другим объектам внутри его кластера и удалённости от объектов в других кластерах. Степень близости отражает, насколько хорошо объекты сгруппированы внутри своих кластеров, и насколько различаются между собой разные кластеры.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Индекс Данна (Dunn Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше значение индекса Данна, тем лучше разделены кластеры. Это означает, что межкластерные расстояния максимизированы, а внутрикластерные расстояния минимизированы. Более чёткое разделение кластеров приводит к более высокому значению индекса Данна.\n",
    "\n",
    "В отличие от коэффициента силуэта, индекс Данна не имеет фиксированного диапазона значений, и его интерпретация может быть сложнее. Обычно сравнивают несколько различных кластерных разбиений, и кластеризация с более высоким значением индекса Данна считается более оптимальной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Индекс Дэвиса — Болдина (Davies–Bouldin Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем меньше значение индекса Дэвиса — Болдина, тем лучше разделены кластеры. Это означает, что внутрикластерные схожести максимизированы, а межкластерные различия минимизированы. Более чёткое разделение кластеров приводит к более низкому значению индекса Дэвиса — Болдина.\n",
    "\n",
    "Сложность кластеризации заключается в том, что на одной выборке может быть несколько различных вариантов разделения на кластеры и не всегда ясно, какое разбиение является правильным. Формализация критериев для определения правильного разбиения на практике достаточно сложна, поэтому сама задача кластеризации не всегда имеет однозначное решение.\n",
    "\n",
    "На практике при отсутствии разметки и неизвестном заранее количестве кластеров одна из лучших метрик — коэффициент силуэта. Он позволяет оценить качество кластеризации, принимая во внимание меру компактности кластеров и их отделённость друг от друга. Коэффициент силуэта позволяет выбирать наилучший вариант кластеризации в таких случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распространённые алгоритмы кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество алгоритмов кластеризации. У каждого из них есть свои особенности и возможности применения. Перечислим наиболее распространённые алгоритмы.\n",
    "\n",
    "<b>K-средних (k-means)</b> разбивает данные на k кластеров, где k задаёт пользователь. Алгоритм итеративно оптимизирует положения центроидов кластеров и присваивает объекты к ближайшим центроидам. Это один из самых популярных алгоритмов кластеризации.\n",
    "\n",
    "<b>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</b> основан на плотности данных. Алгоритм ищет плотные области в пространстве данных и формирует кластеры на основе связанных плотных областей. DBSCAN может обнаруживать кластеры произвольной формы и обрабатывать выбросы.\n",
    "\n",
    "<b>Иерархическая кластеризация</b> — это семейство алгоритмов, которые строят иерархическую структуру кластеров. Два основных подхода в иерархической кластеризации — агломеративный и дивизионный.\n",
    "\n",
    "-    Агломеративные методы действуют от частного к общему: начинают с отдельных объектов и последовательно объединяют их в кластеры, пока не будет достигнуто заданное условие остановки.\n",
    "-    Дивизионные методы действуют от общего к частному: начинают с одного крупного кластера и разделяют его на более мелкие, пока не будут получены отдельные кластеры.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Агломеративная кластеризация (agglomerative hierarchical clustering)</b> — это вариант иерархической кластеризации, где начинают с отдельных объектов и последовательно объединяют их на основе среднего расстояния между кластерами. Алгоритм объединяет кластеры, пока не будет достигнуто заданное количество кластеров.\n",
    "\n",
    "<b>GMM (Gaussian Mixture Model)</b> моделирует данные с использованием смеси нормальных (гауссовских) распределений. Каждый компонент смеси соответствует одному кластеру, и модель пытается найти наиболее вероятную смесь, описывающую данные.\n",
    "\n",
    "Сравнение основных алгоритмов кластеризации представлено в таблице ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n",
    ".tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-cly1{text-align:left;vertical-align:middle}\n",
    ".tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}\n",
    ".tg .tg-th5u{background-color:#88CDB2;border-color:inherit;color:#062425;font-weight:bold;text-align:left;vertical-align:top}\n",
    ".tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-th5u\">Метод</th>\n",
    "    <th class=\"tg-th5u\">Основа алгоритма</th>\n",
    "    <th class=\"tg-th5u\">Входные данные</th>\n",
    "    <th class=\"tg-1wig\">Фиксированное количество кластеров</th>\n",
    "    <th class=\"tg-1wig\">Форма кластеров</th>\n",
    "    <th class=\"tg-1wig\">Обнаружение выбросов</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">K-средних</td>\n",
    "    <td class=\"tg-lboi\">Расстояние между объектами и центроидами</td>\n",
    "    <td class=\"tg-lboi\">Фактические наблюдения</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "    <td class=\"tg-cly1\">Сферические кластеры</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">DBSCAN</td>\n",
    "    <td class=\"tg-lboi\">Плотность регионов в данных</td>\n",
    "    <td class=\"tg-lboi\">Фактические наблюдения или попарные расстояния между наблюдениями</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "    <td class=\"tg-cly1\">Произвольная</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">Иерархическая кластеризация</td>\n",
    "    <td class=\"tg-lboi\">Расстояние между объектами</td>\n",
    "    <td class=\"tg-lboi\">Попарные расстояния между наблюдениями</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "    <td class=\"tg-cly1\">Произвольная</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">Агломеративная кластеризация</td>\n",
    "    <td class=\"tg-lboi\">Расстояние между объектами</td>\n",
    "    <td class=\"tg-lboi\">Попарные расстояния между наблюдениями</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "    <td class=\"tg-cly1\">Произвольная</td>\n",
    "    <td class=\"tg-cly1\">Нет</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-1wig\">GMM</td>\n",
    "    <td class=\"tg-cly1\">Смесь гауссовых распределений</td>\n",
    "    <td class=\"tg-cly1\">Фактические наблюдения</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "    <td class=\"tg-cly1\">Сферические кластеры с различными ковариационными структурами</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таблице представлены основные характеристики каждого метода, их применимость в различных ситуациях и преимущества. Выбор алгоритма зависит от характеристик данных, размерности пространства, предполагаемого числа кластеров и других факторов.\n",
    "\n",
    "Также приведём визуальное сравнение качества работы алгоритмов кластеризации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше представлена работа алгоритмов с разными датасетами: вложенные окружности, наборы данных в виде дуг (подков), три близко расположенных друг к другу кластера, три параллельных кластера, три отдалённых друг от друга кластера, один кластер.\n",
    "\n",
    "Качество работы алгоритма измеряется тем, насколько корректно он кластеризует эти данные, и тем, сколько времени он тратит на это. Информация о скорости работы алгоритма представлена в правом нижнем углу.\n",
    "\n",
    "Графическое сравнение алгоритмов показывает, что наиболее качественные результаты кластеризации дают DBSCAN и OPTICS, чуть менее качественный — агломеративная кластеризация. Наиболее сложные датасеты (строки 1, 2 и 6) кластеризуются лучше с помощью DBSCAN и OPTICS, однако DBSCAN более оптимален по скорости работы.\n",
    "\n",
    "Далее мы подробно рассмотрим алгоритмы k-means, DBSCAN и агломеративной кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снижение размерности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Снижение размерности данных —</b> это процесс уменьшения числа признаков (измерений) в наборе данных. Его выполняют для упрощения анализа данных, устранения шума, избавления от избыточной информации или подготовки данных для последующего использования в задачах машинного обучения.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы снижения размерности применяют в различных задачах анализа данных и машинного обучения. Примеры таких задач:\n",
    "\n",
    "- <b>    Визуализация данных</b>. Снижение размерности помогает визуализировать данные высокой размерности в двух или трёх измерениях для лучшего понимания структуры данных, обнаружения паттернов или визуального анализа.\n",
    "- <b>    Отбор признаков</b>. Методы снижения размерности можно использовать для извлечения наиболее информативных признаков из исходных данных. Снижение размерности можно применять перед построением моделей машинного обучения, чтобы уменьшить размерность входных данных и удалить избыточные или неинформативные признаки. Это может улучшить производительность моделей и сократить вычислительную сложность.\n",
    "- <b>    Устранение шума</b>. Методы снижения размерности помогают устранить шум в данных, фильтруя или игнорируя менее значимые компоненты.\n",
    "- <b>    Сжатие данных</b> для уменьшения расхода памяти.\n",
    "\n",
    "Это только некоторые типы задач, для которых применяют алгоритмы снижения размерности данных. Выбор подходящего метода зависит от конкретной задачи, типа данных и требований анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распространённые алгоритмы снижения размерности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество методов для снижения размерности данных. Рассмотрим некоторые из них.\n",
    "\n",
    "<b>Метод главных компонент (PCA)</b> выполняет линейное преобразование данных, чтобы получить новые некоррелированные переменные, которые называются главными компонентами. PCA — наиболее популярный метод снижения размерности.\n",
    "\n",
    "<b>Метод t-SNE (t-Distributed Stochastic Neighbor Embedding)</b> позволяет визуализировать данные высокой размерности, сохраняя относительные расстояния между объектами. Он стремится сохранить соседство объектов в исходном пространстве и спроецировать их на низкоразмерное пространство для визуализации.\n",
    "\n",
    "<b>Метод линейного дискриминантного анализа (Linear Discriminant Analysis, LDA).</b> Линейный дискриминантный анализ — это статистический метод, используемый для анализа и классификации данных. Он относится к области обучения с учителем, где каждому образцу данных присваивается определённая метка класса.\n",
    "\n",
    "<b>Основная цель LDA</b> — найти линейные комбинации признаков, которые наилучшим образом разделяют различные классы данных. Это достигается путём максимизации разделения между классами и минимизации разброса внутри каждого класса.\n",
    "\n",
    "<b>Автоэнкодеры (autoencoders) </b>— это нейронные сети, которые обучаются восстанавливать входные данные на выходе. Внутри модели создаются представления данных меньшей размерности, которые модель затем пробует восстановить до исходной размерности. Автоэнкодеры можно использовать для снижения размерности и извлечения из данных наиболее информативных признаков.\n",
    "\n",
    "<b>Корреляционный анализ</b> — это статистический метод, который используется для измерения степени связи между двумя или более переменными. Он позволяет определить, насколько тесно связаны две переменные и какие типы связей между ними существуют. В корреляционном анализе используется коэффициент корреляции, который измеряет степень линейной зависимости между переменными.\n",
    "\n",
    "<b>Информативный признаковый отбор (information gain)</b> в алгоритмах на деревьях — это метод выбора наиболее информативных признаков при построении деревьев решений или других алгоритмов, основанных на деревьях, таких как случайный лес.\n",
    "\n",
    "<b>Алгоритм регуляризации L1</b>. Применение L1-регуляризации приводит к решению, при котором некоторые коэффициенты модели становятся равными нулю. Это позволяет выполнить отбор признаков, исключая несущественные признаки из модели. Признаки с нулевыми коэффициентами считаются неинформативными или слабо влияющими на целевую переменную.\n",
    "\n",
    "Сравнение основных алгоритмов снижения размерности данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n",
    ".tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:0px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:0px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-cly1{text-align:left;vertical-align:middle}\n",
    ".tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}\n",
    ".tg .tg-th5u{background-color:#88CDB2;border-color:inherit;color:#062425;font-weight:bold;text-align:left;vertical-align:top}\n",
    ".tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\" style=\"undefined;table-layout: fixed; width: 1229px\">\n",
    "<colgroup>\n",
    "<col style=\"width: 103px\">\n",
    "<col style=\"width: 363px\">\n",
    "<col style=\"width: 338px\">\n",
    "<col style=\"width: 425px\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-th5u\">Метод</th>\n",
    "    <th class=\"tg-th5u\">Описание</th>\n",
    "    <th class=\"tg-th5u\">Преимущества</th>\n",
    "    <th class=\"tg-1wig\">Недостатки</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">PCA</td>\n",
    "    <td class=\"tg-lboi\">Находит новое пространство признаков, состоящее из главных компонент исходных данных.</td>\n",
    "    <td class=\"tg-lboi\">Простота и высокая скорость вычислений</td>\n",
    "    <td class=\"tg-cly1\">Потеря информации о классификации</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">t-SNE</td>\n",
    "    <td class=\"tg-lboi\">Строит карту точек данных в низкоразмерном пространстве, сохраняя локальную структуру.</td>\n",
    "    <td class=\"tg-lboi\">Сохранение локальной структуры данных</td>\n",
    "    <td class=\"tg-cly1\">Вычислительно сложен и может быть медленным для больших наборов данных.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-fymr\">LDA</td>\n",
    "    <td class=\"tg-lboi\">Находит новое пространство признаков, максимизируя разделение классов данных.</td>\n",
    "    <td class=\"tg-lboi\">Учёт информации о классификации</td>\n",
    "    <td class=\"tg-cly1\">Могут возникнуть проблемы с переобучением, когда классов много или данные несбалансированы.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-1wig\">Autoencoders</td>\n",
    "    <td class=\"tg-cly1\">Нейронные сети, которые обучаются восстанавливать входные данные с помощью сжатого представления.</td>\n",
    "    <td class=\"tg-cly1\">Гибкость в моделировании сложных нелинейных отображений данных</td>\n",
    "    <td class=\"tg-cly1\">Обучение требует больше вычислительных ресурсов и времени в&nbsp;&nbsp;сравнении с другими методами снижения размерности. Могут возникнуть&nbsp;&nbsp;проблемы с переобучением.</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таблице представлены основные характеристики методов снижения размерности данных: PCA, t-SNE, LDA и Autoencoders. У каждого метода есть свои особенности, варианты использования и преимущества. Это позволяет выбрать подходящий метод в зависимости от целей и требований конкретной задачи снижения размерности данных.\n",
    "\n",
    "На рисунках ниже представлен пример визуализации датасета Iris с помощью алгоритмов снижения размерности PCA и LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_7_11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы рассмотрим алгоритмы PCA и t-SNE более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Давайте резюмируем.\n",
    "\n",
    "<b>Обучение без учителя (unsupervised learning)</b> — это раздел машинного обучения, в котором модель обучается на неразмеченных данных, то есть на данных, не имеющих заранее определённых меток или целевых переменных. В отличие от обучения с учителем, где модель обучается на размеченных данных с известными метками классов или целевыми значениями, обучение без учителя направлено на обнаружение скрытых структур, закономерностей и интересных паттернов в данных.\n",
    "\n",
    "Кластеризация и снижение размерности — классические подразделы обучения без учителя.\n",
    "\n",
    "<b>Кластеризация</b>:\n",
    "\n",
    " -   Кластеризация — это метод группировки данных в кластеры с похожими объектами.\n",
    " -   Распространённые метрики качества для оценки эффективности кластеризации: среднее внутрикластерное расстояние, среднее межкластерное расстояние, коэффициент силуэта, индекс Данна, индекс Дэвиса — Болдина.\n",
    " -   Распространённые алгоритмы кластеризации: k-средних, DBSCAN, иерархическая кластеризация, GMM.\n",
    "\n",
    "<b>Снижение размерности</b>:\n",
    "\n",
    " -   Снижение размерности данных — это процесс уменьшения количества признаков в данных.\n",
    " -  Распространённые алгоритмы снижения размерности данных: PCA, t-SNE, LDA и автокодировщики.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
