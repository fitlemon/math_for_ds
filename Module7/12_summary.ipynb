{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Обучение без учителя (Unsupervised learning)</b> — это подраздел машинного обучения, в котором модель обучается на неразмеченных данных без наличия явно заданных целевых переменных или меток. В отличие от обучения с учителем, где модель обучается на парах «объект-ответ», в задачах обучения без учителя нет явно определённых целей или правильных ответов. Основная цель обучения без учителя состоит в извлечении скрытых структур, паттернов или информации из неразмеченных данных.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные задачи обучения без учителя:\n",
    "\n",
    "-<b>    Кластеризация</b>. Задача заключается в разделении набора данных на группы или кластеры, где объекты внутри каждого кластера схожи между собой, а объекты из разных кластеров различаются. Кластеризация помогает выявить скрытую структуру в данных и понять, как объекты могут быть группированы на основе их сходства.<br>\n",
    "-<b>    Снижение размерности</b>. Задача состоит в уменьшении размерности данных путём проецирования их на пространство меньшей размерности. Цель — сохранение важных характеристик и структуры данных с одновременным устранением шума и избыточности. Понижение размерности упрощает визуализацию и анализ данных, а также может улучшить производительность моделей машинного обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные алгоритмы кластеризации:\n",
    "\n",
    "- <b>    K-средних (k-means)</b>. Это один из наиболее популярных и простых алгоритмов кластеризации. Он разбивает набор данных на K кластеров, где каждый объект присваивается ближайшему центроиду. Центроиды пересчитываются на каждой итерации до достижения сходимости.\n",
    "- <b>    Иерархическая кластеризация</b>. Этот алгоритм строит иерархическую структуру кластеров, которую можно представить в виде дендрограммы. Он может быть агломеративным (начинать с каждого объекта в отдельном кластере и постепенно объединять их) или дивизивным (начинать с одного кластера и разделять его на подкластеры).\n",
    "- <b>    DBSCAN (Density-Based Spatial Clustering of Applications with Noise).</b> Этот алгоритм основан на плотности данных. Он определяет кластеры, исходя из плотности объектов в их окрестности. DBSCAN может обнаруживать кластеры произвольной формы и имеет возможность обнаруживать выбросы.\n",
    "- <b>    GMM (Gaussian Mixture Model).</b> Это вероятностная модель, которая моделирует данные как смесь нескольких гауссовских распределений. GMM предполагает, что каждый кластер имеет свое гауссовское распределение и объекты внутри кластера генерируются из этого распределения. В рамках этого курса мы его не рассматриваем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные алгоритмы снижения размерности:\n",
    "\n",
    "- <b>    PCA (Principal Component Analysis).</b> Это один из наиболее распространённых алгоритмов снижения размерности. PCA находит линейные комбинации исходных признаков (главные компоненты), которые объясняют наибольшую дисперсию в данных. После этого можно выбрать первые k главных компонент для сокращения размерности данных.\n",
    "- <b>    t-SNE (t-Distributed Stochastic Neighbor Embedding).</b> Этот алгоритм используется для визуализации и снижения размерности данных. Он стремится сохранить локальные сходства объектов, перенося их на пространство меньшей размерности. Алгоритм t-SNE обычно хорошо работает для сохранения глобальных структур данных, но у него могут возникать трудности с сохранением глобальных расстояний.\n",
    "- <b>    UMAP (Uniform Manifold Approximation and Projection).</b> Это относительно новый метод снижения размерности, который также используется для визуализации данных. UMAP стремится сохранить глобальную структуру данных и их локальные сходства. Он основан на построении графа соседей и оптимизации распределения объектов на меньшую размерность с сохранением сходства соседей. В рамках этого курса мы его не рассматриваем.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
