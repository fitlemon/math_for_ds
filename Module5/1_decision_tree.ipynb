{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель занятия — познакомиться с деревьями решений и алгоритмом их построения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы рассматривали метрические (метод ближайших соседей) и линейные (линейная регрессия, логистическая регрессия, SVM) модели. Теперь давайте поговорим о новом классе моделей, которые хорошо справляются с нелинейными зависимостями — о решающих деревьях.\n",
    "\n",
    "Метрические и линейные модели предполагают линейную независимость признаков, что далеко не всегда хорошо. Например, в задаче предсказания цены на недвижимость увеличение расстояния от метро далеко не всегда будет означать уменьшение стоимости квадратного метра. Если мы рассмотрим дополнительный признак, площадь объекта недвижимости, то для некоторых объектов увеличение расстояния до метро и увеличение площади объекта будет означать, что он относится к загородной недвижимости. Соответственно, цена за квадратный метр у такой недвижимости может и повыситься.\n",
    "\n",
    "В метрических и линейных моделях можно строить новые признаки на основе комбинаций двух или нескольких признаков, однако это вызывает снижение интерпретируемости модели и повышение её вычислительной сложности. Поэтому используется другой подход — алгоритм решающего дерева, который во многом похож на процедуру принятия решения человеком-экспертом.\n",
    "\n",
    "Рассмотрим этот подход подробнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание алгоритма решающего дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Решающее дерево</b> — это древовидная модель машинного обучения, которая представляет собой дерево, где каждый узел — это тест на значение признака, а ветви — возможный результат этого теста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы определить, какой путь (ребро) в дереве выбрать, используются логические правила."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Логические правила</b> (также их называют предикатами или индикаторами) в решающих деревьях — это условия, которые определяют, какие действия должны быть выполнены на каждом узле дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое правило состоит из предиката (условия на признаки) и соответствующей операции принятия решения. Например, для классификации объектов на основе признаков правило может быть записано так: «Если значение признака X1 меньше 5, а значение признака X2 больше 10, то идём в левую ветвь (объект принадлежит к классу A). Иначе идём в правую ветвь (объект принадлежит к классу B)». Здесь предикат — «значение признака X1 меньше 5, а значение признака X2 больше 10». Операция принятия решения — «объект принадлежит к классу A».\n",
    "\n",
    "Логические правила в решающих деревьях могут быть с одним или несколькими условиями. Они помогают определить, какой путь в дереве следует выбрать для получения правильного результата.\n",
    "\n",
    "Условие на предикат можно записать так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\left[x_i \\leq t\\right] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где $ t$  — порог по определённому признаку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, ничего не мешает использовать в предикатах сложные модели и условия. Но в этом обычно нет необходимости, и в качестве предиката берут сравнение значения по одному определённому признаку с порогом.\n",
    "\n",
    "Опишем устройство решающего дерева на примере наглядного набора данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая верхняя вершина называется корневой. Другие вершины с предикатами называются внутренними.\n",
    "\n",
    "Логическое правило для корневой вершины формулируется так: «X1<= 5.0». Всего в этой вершине рассматривается 16 объектов (samples = 16), поделённых на два класса ([8, 8]).\n",
    "\n",
    "Если предиктор «X1<=5.0» для объекта равен True, то объект отправляется в левую вершину, иначе — в правую. После обработки объектов в левой вершине станет четыре объекта класса «Синий класс».\n",
    "\n",
    "Левая вершина называется листовой: для неё уже нет предикторов — только итоговый прогноз для части выборки. В дереве решений это будет тупиковая, конечная ветка, которая не ведёт к новым условиям — она выглядит как лист на дереве, поэтому и зовётся листовой. В случае классификации в листовой вершине присваивается метка самого популярного класса или вероятность класса (равная доле объектов класса в вершине), в случае регрессии — выдаётся константный прогноз (усреднённый по объектам из обучающей выборки, попавшим в данную вершину).\n",
    "\n",
    "Для правого поддерева процесс деления выборки с помощью предикатов продолжится, пока все вершины не станут листовыми.\n",
    "\n",
    "Построение разделяющей поверхности для этого примера показано на рисунках ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">По сути, решающее дерево разбило всё пространство признаков на прямоугольные области. В каждой определённой области объекту присваивается определённый класс.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии информативности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора предикатов используют критерии информативности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Критерии информативности</b> — это метрики, которые используют при построении моделей машинного обучения для оценки качества разделения данных на разные классы или значения (в случае регрессии). В контексте деревьев решений критерии информативности используют для выбора наилучшего разбиения признаков на узлах дерева.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные критерии информативности:\n",
    "\n",
    "-        Для классификации — энтропия и критерий Джини.\n",
    "-        Для регрессии — дисперсия.\n",
    "\n",
    "Энтропия — это мера неопределённости в системе.\n",
    "\n",
    "Она выражается формулой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(X)=-\\sum_{i=1}^c p_i \\log _2\\left(p_i\\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где:\n",
    "\n",
    " - $ X$      — признак;\n",
    "\n",
    "- $ C$         — множество классов;\n",
    "\n",
    "- $p_i$        — вероятность появления класса .\n",
    "\n",
    "Чем больше энтропия, тем больше неопределённости в системе.\n",
    "\n",
    "- Если все значения переменной $X $ равновероятны, то энтропия будет максимальной.\n",
    "- Если все значения имеют одну и ту же величину, энтропия будет минимальной.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Критерий Джини (Gini impurity)</b> также измеряет неопределённость выборки и может использоваться вместо энтропии.\n",
    "\n",
    "Формула для расчёта критерия Джини:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\operatorname{Gini}(X)=1-\\sum_{i=1}^c p_i^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где:\n",
    "\n",
    " - $ X$      — признак;\n",
    "\n",
    "- $ C$         — множество классов;\n",
    "\n",
    "- $p_i$        — вероятность появления класса ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">Значение критерия Джини указывает, насколько однородны объекты в выборке: чем оно больше, тем более однородны объекты.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании критерия Джини в деревьях решений выбирают тот признак, который позволяет наиболее эффективно уменьшить значение критерия Джини при разбиении выборки на подгруппы.\n",
    "\n",
    "При построении дерева решений мы пытаемся разбить данные на подгруппы, где каждая группа будет иметь меньшую энтропию или меру Джини, чем исходная группа. Таким образом, критерий информативности в деревьях можно определить как разность между энтропией или критерием Джини исходного узла и суммой энтропий или критериев Джини полученных поддеревьев.\n",
    "\n",
    "Смысл критериев информативности можно продемонстрировать диаграммой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Дисперсия</b> — это критерий информативности для задач регрессии в деревьях решений. Критерий дисперсии позволяет оценить, насколько хорошо определённый признак разделяет целевую переменную.\n",
    "\n",
    "Формула для расчёта критерия дисперсии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\operatorname{Var}(X)=\\frac{1}{n} \\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где:\n",
    "\n",
    "- $n$        — количество объектов в выборке:\n",
    "\n",
    "- $x_i$        — значение целевой переменной для $i$-го объекта;\n",
    "\n",
    "- $\\bar{x} $        — среднее значение целевой переменной в выборке.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">\n",
    "\n",
    "При разбиении выборки на подгруппы по значению определённого признака происходит уменьшение значения дисперсии в каждой из подгрупп. Информативность признака оценивается как разница между начальным значением дисперсии и суммой дисперсий в подгруппах, взвешенной на количество объектов в каждой подгруппе.\n",
    "\n",
    "Выбирают тот признак, который позволяет максимально уменьшить значение дисперсии при разбиении выборки на подгруппы. Этот признак становится корневым узлом дерева. Процесс разбиения продолжается для каждой подгруппы до тех пор, пока не будет достигнут критерий остановки.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс построения дерева для классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как происходит процесс построения дерева на примере энтропии.\n",
    "\n",
    "Обычно дерево строят с использованием жадного алгоритма: выбирают оптимальные для уровня признак и порог на этом признаке только на этом узле, без попыток просчитать несколько шагов вперёд. Построение дерева происходит с помощью прироста информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Прирост информации (information gain)</b> в деревьях решений — это мера для определения того, какой признак лучше всего разделяет данные на различные классы или категории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В деревьях решений признаки используют для разделения данных на более чистые группы, каждая из которых относится к определённому классу или категории. Цель в том, чтобы создать дерево решений, которое максимизирует объём правильно классифицированных данных.\n",
    "\n",
    "Information gain измеряет изменение энтропии (меры неопределённости) или критерия Джини в данных, которое появляется при разделении данных на основе конкретного признака. Таким образом, прирост информации показывает, какой признак наиболее полезен для разделения данных на более чистые группы, которые максимально отличаются друг от друга по классам или категориям. Чем выше прирост информации, тем более полезен признак для разделения данных на более чистые группы.\n",
    "\n",
    "Построение дерева включает в себя следующие шаги:\n",
    "\n",
    "1.        Начните с полного набора данных. Это будет корневой узел дерева.\n",
    "\n",
    "2.        Для каждого признака рассчитайте энтропию (меру неопределённости) набора данных, используя формулу энтропии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(X)=-\\sum_{i=1}^c p_i \\log _2\\left(p_i\\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.       Для каждого признака рассчитайте прирост информации (information gain), используя формулу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ I G(S, A)=H(S)-\\Sigma \\frac{\\left|S_v\\right|}{|S|} * H\\left(S_v\\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где:\n",
    "\n",
    "- $ I G(S, A)$ — прирост информации для признака ;\n",
    "\n",
    "- $ |S_v| $ — количество элементов в поднаборе данных, где значение признака  равно ;\n",
    "\n",
    "- $ |S| $ — общее количество элементов в наборе данных;\n",
    "\n",
    "- $ H (S_v) $ — энтропия поднабора данных, где значение признака равно .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.     Выберите признак с наибольшим приростом информации. Этот признак станет узлом дерева, и каждое его значение можно будет рассматривать как отдельное поддерево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.      Для каждого значения выбранного признака повторите шаги 2–4, пока не будет достигнут критерий остановки: например, минимальное количество элементов в листе или глубина дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Построение дерева с помощью прироста информации для регрессии аналогично его построению для классификации, но использует другую меру неопределённости. Вместо энтропии используется дисперсия.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общий процесс построения дерева с помощью прироста информации для регрессии:\n",
    "\n",
    "1.        Начните с корневого узла, содержащего все обучающие данные.\n",
    "\n",
    "2.        Для каждого признака вычислите прирост информации или изменение дисперсии, которое будет получено при разбиении данных на основе этого признака, используя формулу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ I G(S, A)=\\operatorname{Var}(S)-\\sum \\frac{\\left|S_v\\right|}{|S|} * \\operatorname{Var}\\left(S_v\\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где:\n",
    "\n",
    "- $ I G(S, A)$ — прирост информации для признака ;\n",
    "\n",
    "- $ |S_v| $ — количество элементов в поднаборе данных, где значение признака  равно ;\n",
    "\n",
    "- $ |S| $ — общее количество элементов в наборе данных;\n",
    "\n",
    "- $ Var (S_v) $ — дисперсия поднабора данных, где значение признака равно .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.      Выберите признак с наибольшим приростом информации или минимальной дисперсией.\n",
    "4.      Разделите данные на основе выбранного признака, создавая два дочерних узла.\n",
    "5.      Рекурсивно повторяйте шаги 2–4 для каждого дочернего узла до выполнения некоторого условия остановки: например, до максимальной глубины дерева или минимального числа объектов в узле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение дерева с помощью прироста информации может приводить к появлению неоптимальных разбиений, особенно в случае, когда есть множество признаков с высокой корреляцией. Для уменьшения риска переобучения можно использовать регуляризацию деревьев и ансамбли деревьев, такие как случайный лес (Random Forest) или градиентный бустинг деревьев (Gradient Boosted Trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Регуляризация деревьев решений</b> — это процесс добавления дополнительных ограничений на дерево решений, чтобы уменьшить переобучение модели. Как правило, деревья решений склонны к переобучению, если имеют слишком много листьев и слишком глубоко ветвятся на тренировочных данных. Это может привести к неспособности модели обобщать на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют несколько способов регуляризации деревьев решений:\n",
    "\n",
    "1.        Ограничение глубины дерева (max_depth) — это ограничение на максимальную глубину дерева. Если дерево достигает максимальной глубины, оно перестаёт ветвиться и превращается в лист, что предотвращает переобучение.\n",
    "2.        Минимальное количество элементов в листе (min_samples_leaf) — это ограничение на минимальное количество обучающих элементов, которые должны быть в каждом листе дерева. Если количество элементов в листе меньше заданного значения, он не будет разветвляться, что уменьшает переобучение.\n",
    "3.        Максимальное количество листьев (max_leaf_nodes) — это ограничение на максимальное количество листьев в дереве. Если дерево достигает максимального количества листьев, оно не будет больше разветвляться, что предотвращает переобучение.\n",
    "4.        Регуляризация путём снижения веса (weight regularization) — в этом методе добавляется штраф к функции потерь модели за большие веса, что приводит к более простым моделям и уменьшает переобучение.\n",
    "5.        Подрезка деревьев (pruning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\"><b>Подрезка деревьев (прунинг)</b> — это процесс уменьшения размера дерева решений путём удаления тех его частей, которые не приносят значимого улучшения качества предсказаний.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_5_7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid white; padding: 5px; margin-right: auto;  width: 80%;\">Основная идея прунинга заключается в том, что большие деревья решений могут слишком сильно подстраиваться под обучающие данные и, следовательно, переобучаться. Удаление части дерева может привести к уменьшению переобучения и улучшению обобщающей способности модели.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные методы прунинга деревьев решений:\n",
    "\n",
    "- Pre-pruning означает остановку роста дерева решений до достижения определённой глубины или количества листьев.\n",
    "- Post-pruning — процесс подрезки дерева после того, как оно было полностью построено.\n",
    "- Reduced error pruning — метод, который удаляет узлы из дерева решений, если это приводит к уменьшению ошибок на отложенной выборке.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Применение прунинга деревьев решений позволяет улучшить обобщающую способность модели, уменьшить её сложность и снизить вероятность переобучения. Однако важно найти правильный баланс между уменьшением размера дерева и сохранением информации, которую оно содержит.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бороться с переобучением также помогают композиции деревьев — мы рассмотрим их в следующем юните."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
