{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель занятия — познакомиться с алгоритмом XGBoost, который решает проблему переобучения путём введения регуляризации и оптимизации функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуальная демонстрация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>XGBoost (eXtreme Gradient Boosting)</b> — это алгоритм машинного обучения, основанный на градиентном бустинге деревьев решений. Он был разработан в 2014 году и является одним из наиболее эффективных алгоритмов для задач классификации, регрессии и ранжирования.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost обучает ансамбль деревьев решений последовательно, приближаясь к оптимальному прогнозу с каждой новой итерацией. На каждой итерации алгоритм добавляет новое дерево, которое предсказывает остатки предыдущих деревьев. Таким образом, каждое новое дерево корректирует ошибки предыдущих деревьев.\n",
    "\n",
    "По умолчанию XGBoost строит деревья level-wise, то есть строит все узлы на одном уровне перед переходом к следующему уровню."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако можно строить деревья по листьям (leaf-wise), когда каждый узел строится так, чтобы максимизировать уменьшение функции потерь. Это может способствовать более быстрому обучению и увеличению точности моделей, но в некоторых случаях может вызывать переобучение.\n",
    "\n",
    "При выборе стратегии построения деревьев необходимо учитывать особенности конкретной задачи и набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Основные компоненты алгоритма XGBoost:</b>\n",
    "\n",
    "-    <b>Функция потерь </b>определяет, какой функционал оптимизируется в процессе обучения. В XGBoost используется функция потерь, которая состоит из двух частей: функции потерь для задачи (например, MSE для регрессии или логистическая функция потерь — для классификации) и штрафа за сложность модели (регуляризация).\n",
    "-    <b>Деревья решений</b>. В качестве базовых алгоритмов используются решающие деревья. Каждое дерево строится на основе взвешенной версии обучающего набора данных, которая зависит от остатков предыдущих деревьев.\n",
    "-   <b> Регуляризация.</b> XGBoost поддерживает несколько методов регуляризации, включая L1- и L2-регуляризацию, ограничение глубины деревьев, ограничение на число листьев, ограничение на минимальное количество выборок в листе и ограничение на максимальное количество признаков при поиске наилучшего разделения. Регуляризация помогает предотвратить переобучение модели и повысить её обобщающую способность.\n",
    "- <b>    Градиентный спуск.</b> Для нахождения оптимальных весов модели XGBoost использует градиентный спуск. На каждой итерации градиентного спуска алгоритм вычисляет градиент функции потерь по отношению к весам модели и изменяет их с определённым шагом в направлении уменьшения функции ошибки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных для алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаги по предобработке наборов данных для использования с алгоритмом XGBoost:\n",
    "\n",
    "-    <b>Обработка выбросов и пропущенных значений</b>. XGBoost может работать с данными, содержащими пропущенные значения и выбросы, но для достижения лучшей производительности рекомендуется предварительно обработать эти аномалии.\n",
    "-    <b>Обработка категориальных признаков</b>. XGBoost поддерживает категориальные признаки, но их нужно предварительно закодировать в числовом формате. Существуют различные методы кодирования категориальных признаков, такие как One-Hot Encoding, Label Encoding, и выбор метода зависит от конкретной задачи.\n",
    " -   <b>Сбалансированность классов</b>. Если классы в наборе данных несбалансированные, может потребоваться сбалансировать данные. Например, можно использовать взвешивание классов или сэмплирование данных, чтобы уравнять количество примеров в каждом классе.\n",
    " -   <b>Отбор признаков</b>. XGBoost может работать с большим количеством признаков, но для предотвращения переобучения модели может быть полезно отбирать только наиболее важные из них. Это можно сделать с помощью методов отбора признаков, таких как Recursive Feature Elimination (RFE) или SelectKBest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Это не полный список способов, которыми можно обрабатывать данные для XGBoost, но мы привели наиболее распространённые методы. В целом, подготовка данных для использования с XGBoost зависит от конкретного набора данных и задачи машинного обучения.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс обучения\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея алгоритма XGBoost состоит в том, чтобы строить деревья решений последовательно, учитывая ошибки предыдущих деревьев, и объединять их в итоговую модель. Каждое новое дерево обучается на ошибках предыдущих деревьев, чтобы исправить эти ошибки и улучшить качество модели. Этот процесс продолжается до тех пор, пока не будет достигнута определённая степень точности или не будут исчерпаны все ресурсы.\n",
    "\n",
    "<b>Основные шаги обучения алгоритма XGBoost:</b>\n",
    "\n",
    "- <b>Инициализация модели</b>. Начальное значение целевой переменной устанавливается как среднее значение обучающей выборки.\n",
    "- <b>Вычисление градиента и гессиана.</b> Для каждого объекта в обучающей выборке вычисляются градиент и гессиан целевой функции. Градиент представляет собой первую производную функции ошибки, а гессиан — вторую производную. Градиент ошибки — это вектор первых частных производных функции ошибки по каждому параметру модели. Гессиан — это матрица вторых частных производных функции ошибки по каждой паре параметров модели. Градиент и гессиан используют для обучения каждого нового дерева решений.\n",
    "- <b>Обучение дерева</b>. Каждое новое дерево обучается на остатках (разнице между целевой переменной и предсказанным значением) предыдущих деревьев. Для этого используют метод градиентного бустинга, где дерево решений на каждой итерации обучается минимизировать целевую функцию, которую можно выбрать из различных вариантов в зависимости от задачи.\n",
    "- <b>Вычисление весов деревьев</b>. Деревья решений имеют различные веса, которые определяют их важность в модели. Веса деревьев вычисляются на основе ошибок, которые они исправляют, и регуляризации, которая учитывает сложность модели.\n",
    "- <b>Обновление целевой переменной</b>. Целевая переменная обновляется путём вычитания предсказанного значения из текущего значения, что позволяет модели сфокусироваться на ошибках, которые ещё не исправлены.\n",
    "- <b>Повторение шагов.</b> Шаги 2–6 повторяются до тех пор, пока не будет достигнута заданная точность или не будет исчерпано максимальное количество итераций.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Особенности обучения XGBoost:</b>\n",
    "\n",
    "- <b>Обработка отсутствующих значений</b>. XGBoost поддерживает обработку отсутствующих значений, но лучше делать предобработку данных.\n",
    "- <b>Регуляризация</b>. XGBoost использует регуляризацию для контроля сложности модели и во избежание переобучения.\n",
    "- <b>Подбор гиперпараметров</b>. XGBoost позволяет настраивать различные гиперпараметры, такие как глубина дерева, скорость обучения и количество деревьев. Это позволяет найти оптимальные настройки модели для конкретной задачи.\n",
    "- <b>Автоматический выбор признаков</b>. XGBoost оценивает важность признаков на основе их вклада в улучшение целевой функции и может автоматически выбирать наиболее важные из них для моделирования.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важные параметры при обучении XGBoost:\n",
    "\n",
    "- <b>learning_rate </b>— скорость обучения, которая определяет, насколько сильно корректируются веса при обновлении модели на каждой итерации. Выбор этого параметра влияет на скорость сходимости модели и её способность к обобщению.\n",
    "- <b>max_depth</b> — максимальная глубина дерева, которая определяет количество уровней дерева решений. Выбор этого параметра влияет на способность модели к обобщению и на скорость обучения.\n",
    "- <b>min_child_weight </b>— минимальная величина, которую должен иметь вес дочернего узла, чтобы продолжать делить узел. Выбор этого параметра влияет на устойчивость модели к шуму и на её способность к обобщению.\n",
    "- <b>gamma</b> — минимальное снижение функции ошибки, которое необходимо достичь, чтобы продолжать делить узел. Выбор этого параметра влияет на устойчивость модели к шуму и на скорость обучения.\n",
    "- <b>subsample</b> — доля выборки, используемая для обучения каждого дерева. Выбор этого параметра влияет на способность модели к обобщению и на устойчивость к переобучению.\n",
    "- <b>colsample_bytree </b>— доля признаков, используемых для обучения каждого дерева. Выбор этого параметра влияет на способность модели к обобщению и на устойчивость к переобучению.\n",
    "- <b>alpha</b> — коэффициент L1-регуляризации весов деревьев. Выбор этого параметра влияет на скорость обучения и на способность модели к обобщению.\n",
    "- <b>lambda</b> — коэффициент L2-регуляризации весов деревьев. Выбор этого параметра влияет на скорость обучения и на способность модели к обобщению.\n",
    "- <b>num_boosted_round</b> — количество итераций обучения. Выбор этого параметра влияет на скорость обучения и на способность модели к обобщению.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Выбор оптимальных значений этих гиперпараметров может существенно повлиять на точность и обобщающую способность модели XGBoost.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества алгоритма XGBoost часто используют следующие метрики:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Accuracy:</b> $\\frac{TP+TN}{TP+TN+FP+FN}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Precision:</b>$\\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Recall:</b>$\\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>F1: </b> $2* \\frac{precision*recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE\n",
    "- MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерпретация признаков с помощью алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В XGBoost есть несколько методов для интерпретации важности признаков. Они помогают понять, какие признаки вносят наибольший вклад в модель.\n",
    "\n",
    "Рассмотрим несколько таких методов:\n",
    "\n",
    "-    <b>Важность признаков на основе деревьев</b>. Этот метод основан на том, как деревья в модели XGBoost используют признаки для принятия решений. Для каждого признака суммируется значение важности, которое это свойство получает при разделении данных на каждом узле дерева. Чем выше суммарное значение важности, тем важнее признак.\n",
    "-    <b>SHAP-значения</b>. SHAP (SHapley Additive exPlanations) — это метод, который предоставляет объяснение для каждого предсказания, показывая, какой вклад вносит каждый признак в конечный результат. Для распределения вклада между признаками метод использует теорию игр Шепли.\n",
    "-    <b>Partial Dependence Plot (PDP).</b> PDP — это график, который показывает, как модель меняет свои прогнозы в зависимости от значений одного признака. При этом все остальные признаки не меняются. Это позволяет понять, какой эффект на прогноз оказывает каждый отдельный признак.\n",
    "-    <b>Feature Interaction</b>. XGBoost также позволяет выявлять взаимодействия между признаками. Это может быть полезно, когда важность признаков взаимозависима и один признак вносит больший вклад только вместе с другими признаками.\n",
    "\n",
    "Все эти методы помогают понять, какие признаки вносят наибольший вклад в модель XGBoost и как они взаимодействуют между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost — это мощный алгоритм машинного обучения, который применяют во многих областях.\n",
    "\n",
    "Примеры применения:\n",
    "\n",
    " -   Классификация — в медицине, финансах, при обработке естественного языка и т. д.\n",
    " -   Регрессия — в задачах прогнозирования цен на недвижимость, оценки риска финансовых инструментов и т. д.\n",
    " -   Ранжирование — для построения рекомендательных систем, ранжирования результатов поиска и т. д.\n",
    " -   Анализ данных — для поиска закономерностей и трендов в данных, а также для выявления аномалий и выбросов.\n",
    " -   Обработка изображений — для классификации изображений, определения объектов на изображении, обнаружения дефектов и т. д.\n",
    " -   Обработка звука — для классификации звуковых сигналов, распознавания речи, анализа звукового спектра и т. д.\n",
    " -   Промышленность — для оптимизации управления производственными линиями, логистики и т. д.\n",
    "\n",
    "XGBoost широко применяют в различных областях, где требуется высокая точность и скорость работы алгоритма машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Плюсы и минусы алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \t😃 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Высокая производительность и масштабируемость на больших наборах данных.\n",
    "- Превосходные результаты на различных задачах машинного обучения.\n",
    "- Поддержка распределённых вычислений для обучения на кластерах.\n",
    "- Реализация регуляризации для борьбы с переобучением.\n",
    "- Возможность обработки разных типов данных (числовых, категориальных, текстовых и т. д.).\n",
    "- Возможность обработки пропущенных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Большое количество гиперпараметров может быть сложным для настройки.\n",
    "-    Не так хорошо работает на маленьких выборках данных.\n",
    "-    Может потребоваться больше времени для обучения, чем для простых моделей машинного обучения, таких как линейные модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost реализован в библиотеке <a href=\"https://xgboost.readthedocs.io/en/stable/\">xgboost</a>. Для задачи классификации в библиотеке существует классификаторXGBClassifier, а для задач регрессии — XGBRegressor.\n",
    "\n",
    "Основные параметры этих моделей:\n",
    "\n",
    "    n_estimators — количество деревьев в лесу. По умолчанию n_estimators=100.\n",
    "    objective — функция потерь, которую требуется минимизировать при обучении модели. Поддерживаются различные функции потерь в зависимости от задачи (например, binary:logistic для бинарной классификации или reg:squarederror — для регрессии). По умолчанию objective=reg:squarederror.\n",
    "    max_depth — максимальная глубина каждого дерева в лесу. По умолчанию max_depth=6.\n",
    "    learning_rate — шаг обучения (также называемый темпом обучения). По умолчанию learning_rate=0.3.\n",
    "    min_child_weight — минимальный вес, необходимый для разделения узла. По умолчанию min_child_weight=1.\n",
    "    subsample — доля обучающих данных, которые используются для каждого дерева. По умолчанию subsample=1.\n",
    "    colsample_bytree — доля признаков, которые используются для каждого дерева. По умолчанию colsample_bytree=1.\n",
    "    reg_alpha — коэффициент L1-регуляризации. По умолчанию reg_alpha=0.\n",
    "    reg_lambda — коэффициент L2-регуляризации. По умолчанию reg_lambda=1.\n",
    "\n",
    "Классы XGBClassifier и XGBRegressor также имеют методы:\n",
    "\n",
    "    fit(X, y) — для обучения модели на данных X и y;\n",
    "    predict(X) — для предсказания целевых значений для новых данных X;\n",
    "    score(X, y) и get_params() — для получения оценки точности модели и параметров модели соответственно.\n",
    "\n",
    "Кроме того, класс XGBClassifier имеет метод predict_proba(X), который возвращает вероятности принадлежности каждому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 1px solid white; padding: 5px; margin-right: auto;  width: 80%;\">\n",
    "Давайте резюмируем:\n",
    "\n",
    "-    XGBoost (Extreme Gradient Boosting) — это эффективный и мощный алгоритм машинного обучения, используемый для задач классификации и регрессии.\n",
    "-    Основные преимущества XGBoost — высокая скорость обучения и предсказания, возможность работы с большими объёмами данных, устойчивость к переобучению и возможность интерпретации результатов.\n",
    "-    В XGBoost используется ансамбль деревьев решений, которые объединяются с помощью градиентного бустинга. Этот метод позволяет улучшать качество модели, последовательно добавляя новые деревья и корректируя ошибки предыдущих.\n",
    "-    XGBoost предлагает множество параметров для настройки модели, включая параметры дерева и параметры бустинга. Они позволяют контролировать глубину деревьев, скорость обучения, количество деревьев в ансамбле и другие факторы, влияющие на качество модели.\n",
    "-    В целом, XGBoost является одним из наиболее эффективных и гибких инструментов машинного обучения, которые можно использовать для решения различных задач классификации и регрессии.\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
