{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2369EvqoDAR9"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpbZzAAZDASA",
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности реализаций градиентного бустинга xgboost, lightgbm и catboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KOy94-ZDASD"
   },
   "source": [
    "Мы будем использовать датасет \"Bank Marketing\". В датасете \"Bank Marketing\" обычно содержатся данные, связанные с маркетинговыми кампаниями банка. Вот некоторые общие колонки, которые могут присутствовать в датасете \"Bank Marketing\":\n",
    "\n",
    "1. **age**: Возраст клиента.\n",
    "\n",
    "2. **job**: Тип работы клиента.\n",
    "\n",
    "3. **marital**: Семейное положение клиента.\n",
    "\n",
    "4. **education**: Образование клиента.\n",
    "\n",
    "5. **default**: Есть ли у клиента кредитный дефолт (задолженность).\n",
    "\n",
    "6. **balance**: Баланс на счете клиента.\n",
    "\n",
    "7. **housing**: Имеет ли клиент ипотеку (жилую недвижимость).\n",
    "\n",
    "8. **loan**: Имеет ли клиент личный заем.\n",
    "\n",
    "9. **contact**: Способ связи с клиентом.\n",
    "\n",
    "10. **day**: День месяца, когда был выполнен последний контакт.\n",
    "\n",
    "11. **month**: Месяц года, когда был выполнен последний контакт.\n",
    "\n",
    "12. **duration**: Продолжительность последнего контакта в секундах.\n",
    "\n",
    "13. **campaign**: Количество контактов, выполненных во время данной кампании.\n",
    "\n",
    "14. **pdays**: Количество дней, прошедших с момента последнего контакта до предыдущей кампании.\n",
    "\n",
    "15. **previous**: Количество контактов, выполненных перед текущей кампанией.\n",
    "\n",
    "16. **poutcome**: Результат предыдущей маркетинговой кампании.\n",
    "\n",
    "17. **target_variable**: Целевая переменная, которую необходимо предсказать. Например, это может быть информация о том, стал ли клиент подписчиком (1) или нет (0) на определенное предложение банка.\n",
    "\n",
    "Обратите внимание, что фактические названия колонок и их типы данных могут различаться в зависимости от конкретного датасета \"Bank Marketing\", поэтому рекомендуется проверить описание колонок в вашем конкретном наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5TI7cZVDMuN",
    "outputId": "15894ae4-f0af-4ad2-f1b7-b4384dd7e841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: catboost in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (1.2)\n",
      "Requirement already satisfied: hyperopt in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: optuna in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: bayesian-optimization in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from catboost) (5.15.0)\n",
      "Requirement already satisfied: six in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from hyperopt) (3.1)\n",
      "Requirement already satisfied: future in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from hyperopt) (0.18.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from hyperopt) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from hyperopt) (2.2.1)\n",
      "Requirement already satisfied: py4j in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (2.0.17)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: Mako in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from matplotlib->catboost) (3.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from plotly->catboost) (8.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\avglinsky\\pycharmprojects\\math_and_ml_algorithms\\venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn xgboost lightgbm catboost hyperopt optuna bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6Eg2YM-DASE"
   },
   "outputs": [],
   "source": [
    "# Библиотеки общего назначения\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Модули метрик\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Модули препроцессинга данных\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Библиотеки градиентного бустинга\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Загрузка данных из CSV файла\n",
    "url = 'https://datahub.io/machine-learning/bank-marketing/r/bank-marketing.csv'\n",
    "column_names = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', \n",
    "                'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', \n",
    "                'poutcome', 'target_variable']\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data.columns = column_names\n",
    "data.head()\n",
    "\n",
    "# Разделение на признаки (X) и целевую переменную (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable'].apply(lambda x: x - 1)\n",
    "\n",
    "\n",
    "# Разделение на обучающий и тестовый наборы данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора для категориальных и числовых признаков\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Объединение препроцессоров\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Создание пайплайна для препроцессора\n",
    "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Применение препроцессора к данным\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQUyMMjxDASF"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLLW5M1jDASF"
   },
   "source": [
    "XGBoost (eXtreme Gradient Boosting) - это библиотека градиентного бустинга, предназначенная для решения задач классификации и регрессии. Она основана на идее градиентного бустинга, который строит ансамбль слабых моделей (обычно деревьев решений) и объединяет их для получения более сильной и устойчивой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjEzU1FDASG"
   },
   "source": [
    "Описание алгоритма XGBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - Каждое дерево строится поэтапно. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - Для построения дерева используется критерий информативности, такой как критерий Джини или энтропийный критерий, для определения наилучшего разбиения на каждом узле дерева.\n",
    "   - Деревья строятся с ограничением на их глубину или другими параметрами, чтобы избежать переобучения.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева. Веса определяются скоростью обучения (learning rate), которая контролирует влияние каждого дерева на итоговое предсказание модели.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - Дополнительные механизмы регуляризации в XGBoost помогают предотвратить переобучение и улучшить обобщающую способность модели.\n",
    "   - XGBoost предлагает несколько методов регуляризации, таких как L1- и L2-регуляризация (также известные как регуляризация Лассо и ридж), которые помогают контролировать сложность модели и предотвращать переобучение. Эти методы добавляют штрафы к функции потерь, которые зависят от весов модели.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - XGBoost использует адаптивную функцию потерь, которая сочетает в себе различные функции потерь в зависимости от значения целевой переменной. Например, для задачи классификации с двумя классами может использоваться кросс-энтропия ($L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$), а для задачи регрессии - среднеквадратичная ошибка $(\\frac{1}{n}\\sum (y - \\hat{y})^2)$.\n",
    "   \n",
    "7. Ансамблирование деревьев:\n",
    "   - Поскольку XGBoost строит ансамбль из нескольких деревьев, предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели. Модель объединяет прогнозы всех деревьев с учетом их весов, определенных на основе ошибок и значимости.\n",
    "\n",
    "8. Прогнозирование:\n",
    "    - После обучения модели XGBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HZwAApPDASG"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **colsample_bylevel**: Доля признаков, используемых для построения каждого дерева на каждом уровне. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "7. **colsample_bynode**: Доля признаков, используемых для построения каждого дерева в каждом узле разбиения. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "8. **gamma**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение gamma помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "9. **reg_alpha**: Параметр L1 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "10. **reg_lambda**: Параметр L2 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8_sZQSDDASH",
    "outputId": "f8de00b5-c5bf-4302-dab9-00a0d2ce7fe6"
   },
   "outputs": [],
   "source": [
    "# Библиотеки и модули оптимизации\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определение модели XGBoost\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [30]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "xgb_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_gridsearch_execution_time:\", xgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_gridsearch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWSQWzqEDASI",
    "outputId": "afbd674c-9c5a-4601-c281-623d357a8b3e"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "xgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_hyperopt_execution_time:\", xgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "# Лучшие параметры и значение метрики\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_hyperopt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyXPDbY9DASI",
    "outputId": "ea6ef89b-1e59-41a6-9228-42c5108d2512"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "xgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_optuna_execution_time:\", xgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_optuna_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz1EU4dlDASJ",
    "outputId": "0cff63a2-e588-4c23-f12d-d6e3edd3ad9d"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "xgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_bayes_opt_execution_time:\", xgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "xgb_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-мера на тестовом наборе данных:\", xgb_bayes_opt_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeM4ZGD_DASJ"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCCshNyWDASK"
   },
   "source": [
    "Описание алгоритма LightGBM:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - LightGBM использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - LightGBM использует оптимизированную версию алгоритма градиентного бустинга, которая основана на методе обучения по гистограммам. Это позволяет существенно ускорить процесс построения деревьев.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - LightGBM также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - LightGBM поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - LightGBM также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются для получения итогового предсказания модели.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели LightGBM можно использовать для прогнозирования на новых данных, аналогично XGBoost. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ps-1fi5DASK"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **min_split_gain**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение min_split_gain помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "7. **reg_alpha** и **reg_lambda**: Параметры L1 и L2 регуляризации. Они помогают контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5S_6T_wDDASK",
    "outputId": "fa515e60-9828-4224-865e-9558d913b293"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели LightGBM\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.001],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "lgb_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_gridsearch_execution_time:\", lgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zE6TnKb4DASK",
    "outputId": "71269477-feae-4f57-d2b0-186292a9f117"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "lgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_hyperopt_execution_time:\", lgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HiSaVZ4FDASL",
    "outputId": "3473aecb-9833-4f7e-8f3d-9b193c4f6a40"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "lgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_optuna_execution_time:\", lgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_HST1STDASL",
    "outputId": "f0f0220b-f790-4eb8-ab72-9f39d03d9eb2"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "lgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_bayes_opt_execution_time:\", lgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = lgb.LGBMClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "lgb_bayes_opt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2ffJUp-DASL"
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4nLNa6oDASL"
   },
   "source": [
    "Описание алгоритма CatBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель CatBoost с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - CatBoost использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost и LightGBM. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - CatBoost применяет особый подход к кодированию категориальных признаков, называемый симметричным бинарным кодированием, который учитывает взаимодействия между категориями и признаками.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost и LightGBM.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - CatBoost также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost и LightGBM.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - CatBoost поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost и LightGBM.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - CatBoost также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели.\n",
    "   - CatBoost также учитывает веса деревьев на основе их ошибок и значимости, аналогично XGBoost и LightGBM.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели CatBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRmwOfZJDASM"
   },
   "source": [
    "При оптимизации модели CatBoost можно обратить внимание на следующие важные параметры:\n",
    "\n",
    "1. **depth**: Глубина дерева. Этот параметр определяет, насколько глубоким может быть каждое дерево. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр контролирует вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **iterations**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bylevel**: Доля признаков, используемых для построения каждого уровня дерева. Этот параметр контролирует случайную подвыборку признаков для каждого уровня дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **l2_leaf_reg**: L2 регуляризация. Этот параметр помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "7. **random_strength**: Сила случайности для ускорения обучения. Этот параметр контролирует уровень случайности в выборе признаков при построении дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "8. **bagging_temperature**: Температура бэггинга. Этот параметр контролирует степень случайности в выборе объектов для обучения каждого дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "Важно отметить, что конкретные параметры и их значения могут варьироваться в зависимости от конкретной задачи и набора данных. Рекомендуется провести эксперименты и настраивать параметры с использованием кросс-валидации для достижения наилучшей производительности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARxiWMrZDASM",
    "outputId": "10433a48-930f-41b1-e8b2-ef56d8784b46"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели CatBoost\n",
    "model = CatBoostClassifier(silent=True)\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "# param_grid = {\n",
    "#     'depth': [3, 4, 5],\n",
    "#     'learning_rate': [0.1, 0.05, 0.01],\n",
    "#     'n_estimators': [30, 50, 100]\n",
    "# }\n",
    "param_grid = {\n",
    "    'depth': [5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "catboost_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_gridsearch_execution_time:\", catboost_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kV4FOtyDDASM",
    "outputId": "9d94b52e-457a-4f9a-de3c-d372bd09f176"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'depth': hp.choice('depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'iterations': hp.choice('iterations', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "catboost_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'depth': [3, 4, 5][best['depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'iterations': [30, 50, 100][best['iterations']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlBubWUyDASM",
    "outputId": "451efe28-e71f-4e17-a289-00fc32824e6c"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'iterations': trial.suggest_categorical('iterations', [30, 50, 100])\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "catboost_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "catboost_optuna_f1 = f1_score(y_test, model.predict(X_test))\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ9EjWqWDASN",
    "outputId": "a19fbf5f-b9e4-4a93-f26f-8e0af7ae4815"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "catboost_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_bayes_opt_execution_time:\", catboost_bayes_opt_execution_time)\n",
    "\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели с лучшими параметрами на тестовом наборе данных\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = CatBoostClassifier(silent=True, **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "catboost_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "J3cyFO6cDASN",
    "outputId": "3e751a6b-a748-436f-ee5d-0137c878f45c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создание данных для таблицы сравнения по f1_score и времени работы\n",
    "params_match = [\n",
    "    ['XGBoost gridsearch', f'{xgb_gridsearch_f1}', f'{xgb_gridsearch_execution_time}'],\n",
    "    ['XGBoost hyperopt', f'{xgb_hyperopt_f1}', f'{xgb_hyperopt_execution_time}'],\n",
    "    ['XGBoost optuna', f'{xgb_optuna_f1}', f'{xgb_optuna_execution_time}'],\n",
    "    ['XGBoost bayes_opt', f'{xgb_bayes_opt_f1}', f'{xgb_bayes_opt_execution_time}'],\n",
    "    ['LightGBM gridsearch', f'{lgb_gridsearch_f1}', f'{lgb_gridsearch_execution_time}'],\n",
    "    ['LightGBM hyperopt', f'{lgb_hyperopt_f1}', f'{lgb_hyperopt_execution_time}'],\n",
    "    ['LightGBM optuna', f'{lgb_optuna_f1}', f'{lgb_optuna_execution_time}'],\n",
    "    ['LightGBM bayes_opt', f'{lgb_bayes_opt_f1}', f'{lgb_bayes_opt_execution_time}'],\n",
    "    ['Catboost gridsearch', f'{catboost_gridsearch_f1}', f'{catboost_gridsearch_execution_time}'],\n",
    "    ['Catboost hyperopt', f'{catboost_hyperopt_f1}', f'{catboost_hyperopt_execution_time}'],\n",
    "    ['Catboost optuna', f'{catboost_optuna_f1}', f'{catboost_optuna_execution_time}'],\n",
    "    ['Catboost bayes_opt', f'{catboost_bayes_opt_f1}', f'{catboost_bayes_opt_execution_time}'],\n",
    "\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Модель и оптимайзер', 'F1', 'Время'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df.head(len(params_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "PxuCOJzkDASN",
    "outputId": "a65a7eb3-fd85-48f6-c0f0-a5bfd1adfe49"
   },
   "outputs": [],
   "source": [
    "# Создание данных для таблицы\n",
    "params_match = [\n",
    "    ['Скорость обучения (Learning Rate)', 'eta (learning_rate)', 'learning_rate', 'learning_rate'],\n",
    "    ['Количество деревьев (Number of Trees)', 'n_estimators', 'n_estimators', 'iterations'],\n",
    "    ['Максимальная глубина дерева (Maximum Depth)', 'max_depth', 'max_depth', 'max_depth'],\n",
    "    ['Минимальный вес листа дерева, Минимальное количество объектов в листе (Minimum Child Weight)', 'min_child_weight', 'min_child_samples', 'min_data_in_leaf'],\n",
    "    ['Гамма (Gamma)', 'gamma', 'min_split_gain', 'l2_leaf_reg'],\n",
    "    ['Доля выборки объектов (Subsample Ratio)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Доля выборки признаков (Column Subsampling Ratio)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Лямбда (L2-регуляризация)', 'reg_lambda', 'reg_lambda', 'reg_lambda'],\n",
    "    ['Альфа (L1-регуляризация)', 'reg_alpha', 'reg_alpha', 'reg_alpha'],\n",
    "    ['Максимальное изменение шага (Maximum Delta Step)', 'max_delta_step', '-', '-'],\n",
    "    ['Максимальное количество корзин (Maximum Bin Count)', 'max_bin', 'max_bin', 'max_bin'],\n",
    "    ['Частота сэмплирования объектов (Sampling Frequency for LGBM)', '-', 'subsample_freq', '-'],\n",
    "    ['Масштабирование веса положительного класса (Scale the Weight of Positive Class)', 'scale_pos_weight', 'scale_pos_weight', 'scale_pos_weight'],\n",
    "    ['Режим обучения с помощью градиентного спуска (Booster Type)', 'booster', 'boosting_type', 'boosting_type'],\n",
    "    ['Коэффициент сжатия (Subsample)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Размер выборки для построения дерева (Bagging Fraction)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Способ выбора ветвления (Split Criterion)', 'tree_method', 'tree_learner', 'grow_policy'],\n",
    "    ['Функция потерь (Loss Function)', 'objective', 'objective', 'loss_function'],\n",
    "    ['Максимальное количество листьев (Max Leaf Nodes)', '-', 'num_leaves', 'max_leaves']\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Описание параметра', 'XGBoost', 'LightGBM', 'CatBoost'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
