{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности моделей градиентного бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовое дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTree:\n",
    "    \n",
    "    def __init__(self, max_depth=None, criterion=\"entropy\"):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_split_entropy(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий энтропии.\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        def entropy(y):\n",
    "            \"\"\"\n",
    "            Вычисляет энтропию вектора y со значениями дискретных переменных.\n",
    "            Аргументы:\n",
    "            - y: вектор numpy с дискретными значениями.\n",
    "            Возвращает:\n",
    "            - entropy: значение энтропии типа float.\n",
    "            \"\"\"\n",
    "            # Подсчитываем количество каждого уникального значения в y.\n",
    "            _, counts = np.unique(y, return_counts=True)\n",
    "            # Вычисляем вероятность каждого уникального значения.\n",
    "            probs = counts / len(y)\n",
    "            # Вычисляем значение энтропии.\n",
    "            return -np.sum(probs * np.log2(probs))\n",
    "        \n",
    "        best_feature, best_threshold, best_gain = None, None, 0\n",
    "        # Итерируемся по всем признакам.\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Находим уникальные значения признака.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Итерируемся по всем возможным пороговым значениям признака.\n",
    "            for threshold in thresholds:\n",
    "                # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                # Вычисляем значение критерия энтропии для текущего разбиения.\n",
    "                gain = entropy(y) - (len(left_y) / len(y)) * entropy(left_y) \\\n",
    "                                       - (len(right_y) / len(y)) * entropy(right_y)\n",
    "                # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_best_split_gini(X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий Джини.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        # Завершите реализацию функции gini_index \n",
    "\n",
    "        def gini_index(y):\n",
    "\n",
    "            \"\"\"\n",
    "            Вычисляет критерий Джини для вектора y со значениями дискретных переменных.\n",
    "\n",
    "            Аргументы:\n",
    "            - y: вектор numpy с дискретными значениями.\n",
    "\n",
    "            Возвращает:\n",
    "            - gini: значение критерия Джини типа float.\n",
    "\n",
    "            Подсчитываем количество каждого уникального значения в y.\n",
    "            \"\"\"\n",
    "            _, counts = np.unique(y, return_counts=True)\n",
    "            # Вычисляем вероятность каждого уникального значения.\n",
    "            probs = counts / len(y)\n",
    "\n",
    "            if not len(probs):\n",
    "                return 0\n",
    "\n",
    "            criterion  = 1 - np.sum(probs ** 2)\n",
    "            return criterion\n",
    "\n",
    "        best_feature, best_threshold, best_gain = None, None, 0\n",
    "        # Итерируемся по всем признакам.\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Находим уникальные значения признака.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Итерируемся по всем возможным пороговым значениям признака.\n",
    "            for threshold in thresholds:\n",
    "                # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "\n",
    "                gain = gini_index(y) - (len(left_y) / len(y)) * gini_index(left_y) - (len(right_y) / len(y)) * gini_index(right_y)\n",
    "                # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "        \n",
    "    @staticmethod   \n",
    "    def find_best_split_mse(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, \n",
    "        используя критерий среднеквадратичной ошибки (MSE).\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_mse: значение критерия среднеквадратичной ошибки для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                mse = np.mean((left_y - np.mean(left_y))**2) + np.mean((right_y - np.mean(right_y))**2)\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "        return best_feature, best_threshold, best_mse\n",
    "\n",
    "    def fit(self, X, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Обучает дерево регрессии на обучающих данных X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        \"\"\"\n",
    "        if y_pred is not None:\n",
    "            self.tree = self._build_tree(X, y, depth=0, y_pred=y_pred)\n",
    "        else:\n",
    "            self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Выполняет предсказание для входных данных X.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: вектор numpy с предсказанными вещественными значениями.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth, y_pred=None):\n",
    "        \"\"\"\n",
    "        Рекурсивно строит дерево регрессии, используя входные данные X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        - depth: текущая глубина дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий узел дерева.\n",
    "\n",
    "        \"\"\"\n",
    "        # Проверка условий останова по максимальной глубине и другим критериям\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            # Создание листового узла\n",
    "            return self._create_leaf_node(y, y_pred)\n",
    "        \n",
    "        # Нахождение лучшего разбиения по критерию энтропии\n",
    "        if self.criterion == \"entropy\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_entropy(X, y)\n",
    "            \n",
    "        # Нахождение лучшего разбиения по критерию джини\n",
    "        elif self.criterion == \"gini\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_gini(X, y)\n",
    "\n",
    "        # Нахождение лучшего разбиения по критерию mse\n",
    "        elif self.criterion == \"mse\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_mse(X, y)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Следует задать критерий разбиения из списка [\"mse\", \"entropy\"]')\n",
    "\n",
    "        # Проверка условия останова, если не удалось найти лучшее разбиение\n",
    "        if best_feature is None or best_threshold is None:\n",
    "            # Создание листового узла\n",
    "            if y_pred is not None:\n",
    "                return self._create_leaf_node(y, y_pred)\n",
    "            return self._create_leaf_node(y)\n",
    "\n",
    "        # Разделение данных на левое и правое поддеревья\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Рекурсивное построение левого и правого поддеревьев\n",
    "        if y_pred is not None:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1, y_pred[left_indices])\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1, y_pred[right_indices])\n",
    "        else:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        # Создание узла с информацией о лучшем разбиении\n",
    "        node = {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_tree, 'right': right_tree}\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Обходит дерево регрессии для выполнения предсказания на входных данных x.\n",
    "\n",
    "        Аргументы:\n",
    "        - x: вектор numpy с вещественными значениями признаков.\n",
    "        - node: текущий узел дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - value: предсказанное вещественное значение.\n",
    "        \"\"\"\n",
    "        if 'value' in node:\n",
    "            return node['value']\n",
    "\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._traverse_tree(x, node['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node['right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(BasicTree):\n",
    "    def __init__(self, max_depth=None, criterion=\"mse\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект RegressionTree.\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально). \n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка: \n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева регрессии.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел среднего значения целевой переменной.\n",
    "        \"\"\"\n",
    "        return {'value': np.mean(y)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные: Data $\\{(x_i, y_i)\\}_{i=1}^n$\n",
    "\n",
    "Дифференцируемая функция потерь $L(y_i, F(x_i))$ (квадратичная функция потерь как наиболее часто используемая)\n",
    "\n",
    "Шаг 1: Инициализировать модель константным значением: $F_0(x)=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{i=1}^n L(y_i, \\gamma)$, где $\\gamma$ - предсказанное значение. Обычно используется среднее значение.\n",
    "\n",
    "Шаг 2: for $m=1$ to $M$ : (для каждого из M деревьев в ансамбле)\n",
    "\n",
    "(A) Вычисление остатков (residuals) $r_{i m}=-\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$ for $i=1, \\ldots, n$ (i - i-й обучающий пример)\n",
    "\n",
    "(B) Обучить дерево на объектах $r_{\\text {im }}$ и получить $J$ листьев $R_{j m}$, for $j=1 \\ldots J_m$\n",
    "\n",
    "(C) Для каждого листа $j=1 \\ldots J_m$ вычислить предсказание $\\gamma_{j m}=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{x_i \\in R_{i j}} L\\left(y_i, F_{m-1}\\left(x_i\\right)+\\gamma\\right)$\n",
    "\n",
    "(D) Добавить текущее предсказание к уже полученному значению на предыдущих итерациях $F_m(x)=F_{m-1}(x)+\\nu \\sum_{j=1}^{J_m} \\gamma_{j m} I\\left(x \\in R_{j m}\\right)$ , где $\\nu$ - скорость обучения.\n",
    "\n",
    "Шаг 3: Получить итоговые предсказания $F_M(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Инициализируем предсказания базовой модели нулевым значением\n",
    "        predictions = np.full_like(y, 0)\n",
    "        # Инициализируем предсказания базовой модели средним значением\n",
    "#         predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Вычисляем остатки между предсказаниями и истинными значениями\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Создаем экземпляр регрессионного дерева\n",
    "            tree = RegressionTree(max_depth=self.max_depth)\n",
    "\n",
    "            # Обучаем дерево на остатках\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Получаем предсказания для текущего дерева\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Обновляем предсказания ансамбля\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "            # Сохраняем дерево в список базовых моделей\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Инициализируем предсказания нулевым значением\n",
    "        predictions = np.zeros(len(X))\n",
    "#         predictions = np.full(len(X), np.mean(y))\n",
    "\n",
    "        for tree in self.estimators:\n",
    "            # Получаем предсказания для каждого дерева в ансамбле\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Обновляем предсказания ансамбля\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.4600315720374355\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Генерация случайных данных для задачи регрессии\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение модели градиентного бустинга\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оценка качества модели с помощью среднеквадратичной ошибки\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.scatter(X_test, y_test, color='b', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='r', label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация по: https://www.youtube.com/watch?v=StWY5QWMXCw и \n",
    "https://maelfabien.github.io/machinelearning/GradientBoostC/#\n",
    "\n",
    "\"Log odds\" (логарифм шансов) относится к математической функции, которая измеряет отношение вероятности успеха к вероятности неудачи и берет ее логарифм. Она используется для преобразования вероятностей в непрерывный диапазон и представления их в линейном масштабе.\n",
    "\n",
    "Формально, лог odds отношения шансов вычисляется следующим образом:\n",
    "\n",
    "$\n",
    "\\text{log odds} = \\log\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    "где:\n",
    "- log odds - логарифм шансов (log odds),\n",
    "- p - вероятность успеха (или принадлежности к классу 1),\n",
    "- 1-p - вероятность неудачи (или принадлежности к классу 0).\n",
    "\n",
    "Лог odds представляет собой численное значение, которое может быть положительным или отрицательным, в зависимости от отношения вероятностей успеха и неудачи. Эта мера широко используется в статистике и машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные: Data $\\{(x_i, y_i)\\}_{i=1}^n$\n",
    "\n",
    "Дифференцируемая функция потерь $L(y_i, F(x_i))$\n",
    "\n",
    "Шаг 1: Инициализировать модель константным значением: $F_0(x)=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{i=1}^n L(y_i, \\gamma)$\n",
    "\n",
    "Шаг 2: for $m=1$ to $M$ : (для каждого из M деревьев в ансамбле)\n",
    "\n",
    "(A) Вычисление остатков (residuals) $r_{i m}=-\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$ for $i=1, \\ldots, n$ (i - i-й обучающий пример)\n",
    "\n",
    "(B) Обучить дерево на объектах $r_{\\text {im }}$ и получить $J$ листьев $R_{j m}$, for $j=1 \\ldots J_m$\n",
    "\n",
    "(C) Для каждого листа $j=1 \\ldots J_m$ вычислить предсказание $\\gamma_{j m}=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{x_i \\in R_{i j}} L\\left(y_i, F_{m-1}\\left(x_i\\right)+\\gamma\\right)$\n",
    "\n",
    "(D) Добавить текущее предсказание к уже полученному значению на предыдущих итерациях $F_m(x)=F_{m-1}(x)+\\nu \\sum_{j=1}^{J_m} \\gamma_{j m} I\\left(x \\in R_{j m}\\right)$ , где $\\nu$ - скорость обучения.\n",
    "\n",
    "Шаг 3: Получить итоговые предсказания $F_M(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBClassificationRegressionTree(BasicTree):\n",
    "    \n",
    "    def __init__(self, max_depth=None, criterion=\"entropy\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект GBClassificationRegressionTree.\n",
    "        Реализация по: https://www.youtube.com/watch?v=StWY5QWMXCw и \n",
    "        https://maelfabien.github.io/machinelearning/GradientBoostC/#\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально). \n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка: \n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева регрессии.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел среднего значения целевой переменной.\n",
    "        \"\"\"\n",
    "        return {'value': np.sum(y) / np.sum(y_pred * (1 - y_pred))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=None):\n",
    "        \"\"\"\n",
    "        Инициализация класса GradientBoostingClassifier\n",
    "        \n",
    "        Параметры:\n",
    "        - n_estimators: int, количество базовых моделей (деревьев решений)\n",
    "        - learning_rate: float, скорость обучения (шаг градиентного спуска)\n",
    "        - max_depth: int, максимальная глубина деревьев решений\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели GradientBoostingClassifier\n",
    "        \n",
    "        Параметры:\n",
    "        - X: numpy.array, массив признаков\n",
    "        - y: numpy.array, массив целевых значений\n",
    "        \"\"\"\n",
    "        # Инициализация начальных предсказаний средним значением y\n",
    "        y_pred = np.full_like(y, self._sigmoid(np.log(np.count_nonzero(y == 1) / np.count_nonzero(y == 0))), \n",
    "                              dtype=np.float64)\n",
    "\n",
    "#         y_pred = np.full_like(y, 0, dtype=np.float64)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Вычисление остатков (residuals) как разницы между y и предсказаниями\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Создание и обучение базовой модели (дерева решений) на остатках\n",
    "            estimator = GBClassificationRegressionTree(max_depth=self.max_depth)\n",
    "            estimator.fit(X, residuals, y_pred)\n",
    "\n",
    "            # Добавление базовой модели в список\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "            # Обновление предсказаний путем добавления произведения скорости обучения и предсказаний базовой модели\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание классов для новых данных\n",
    "        \n",
    "        Параметры:\n",
    "        - X: numpy.array, массив признаков для предсказания\n",
    "        \n",
    "        Возвращает:\n",
    "        - predictions: numpy.array, массив предсказанных классов (0 или 1)\n",
    "        \"\"\"\n",
    "        # Инициализация предсказаний с константным значением 0.5\n",
    "        y_pred = np.full(len(X), 0.5)\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            # Обновление предсказаний путем добавления произведения скорости обучения и предсказаний базовой модели\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "        # Округление предсказаний и преобразование в целочисленный тип\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Функция сигмоиды\n",
    "        \n",
    "        Параметры:\n",
    "        - x: numpy.array, массив значений\n",
    "        \n",
    "        Возвращает:\n",
    "        - numpy.array, массив значений, преобразованных с помощью сигмоиды\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Генерация синтетических данных\n",
    "X, y = make_classification(n_classes=2, n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение модели GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование классов для тестовой выборки\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Вычисление и вывод точности модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) - это библиотека градиентного бустинга, предназначенная для решения задач классификации и регрессии. Она основана на идее градиентного бустинга, который строит ансамбль слабых моделей (обычно деревьев решений) и объединяет их для получения более сильной и устойчивой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание алгоритма XGBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - Каждое дерево строится поэтапно. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - Для построения дерева используется критерий информативности, такой как критерий Джини или энтропийный критерий, для определения наилучшего разбиения на каждом узле дерева.\n",
    "   - Деревья строятся с ограничением на их глубину или другими параметрами, чтобы избежать переобучения.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева. Веса определяются скоростью обучения (learning rate), которая контролирует влияние каждого дерева на итоговое предсказание модели.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - Дополнительные механизмы регуляризации в XGBoost помогают предотвратить переобучение и улучшить обобщающую способность модели.\n",
    "   - XGBoost предлагает несколько методов регуляризации, таких как L1- и L2-регуляризация (также известные как регуляризация Лассо и ридж), которые помогают контролировать сложность модели и предотвращать переобучение. Эти методы добавляют штрафы к функции потерь, которые зависят от весов модели.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - XGBoost использует адаптивную функцию потерь, которая сочетает в себе различные функции потерь в зависимости от значения целевой переменной. Например, для задачи классификации с двумя классами может использоваться кросс-энтропия ($L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$), а для задачи регрессии - среднеквадратичная ошибка $(\\frac{1}{n}\\sum (y - \\hat{y})^2)$.\n",
    "   \n",
    "7. Ансамблирование деревьев:\n",
    "   - Поскольку XGBoost строит ансамбль из нескольких деревьев, предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели. Модель объединяет прогнозы всех деревьев с учетом их весов, определенных на основе ошибок и значимости.\n",
    "\n",
    "8. Прогнозирование:\n",
    "    - После обучения модели XGBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание алгоритма LightGBM:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - LightGBM использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - LightGBM использует оптимизированную версию алгоритма градиентного бустинга, которая основана на методе обучения по гистограммам. Это позволяет существенно ускорить процесс построения деревьев.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - LightGBM также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - LightGBM поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - LightGBM также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются для получения итогового предсказания модели.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели LightGBM можно использовать для прогнозирования на новых данных, аналогично XGBoost. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание алгоритма CatBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель CatBoost с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - CatBoost использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost и LightGBM. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - CatBoost применяет особый подход к кодированию категориальных признаков, называемый симметричным бинарным кодированием, который учитывает взаимодействия между категориями и признаками.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost и LightGBM.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - CatBoost также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost и LightGBM.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - CatBoost поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost и LightGBM.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - CatBoost также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели.\n",
    "   - CatBoost также учитывает веса деревьев на основе их ошибок и значимости, аналогично XGBoost и LightGBM.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели CatBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Wrangling & EDA with Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
