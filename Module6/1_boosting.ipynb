{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель занятия — познакомиться с алгоритмом бустинга, который помогает строить ансамбли деревьев и избегать недообучения модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое бустинг?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже рассмотрели бэггинг — один из способов построить композицию моделей на деревьях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Бэггинг (bootstrap aggregating)</b> — это метод ансамблирования, при котором на разных подвыборках обучающего набора данных строят несколько независимых моделей, а затем усредняют их результаты. Бэггинг помогает бороться с разбросом (variance), то есть с тем, что результаты модели могут сильно варьироваться в зависимости от обучающей выборки.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Бустинг (boosting)</b> — это тоже метод ансамблирования, но построенный на итеративном улучшении одной базовой модели путём добавления новых моделей, каждая из которых исправляет ошибки предыдущих.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг помогает бороться со смещением (bias) — с тем, что модель может недообучаться и давать неточные предсказания на новых данных.\n",
    "\n",
    "Таким образом, бэггинг и бустинг — два подхода к решению проблемы ансамблирования моделей. Каждый из них борется с определённым типом ошибки модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные отличия между бэггингом и бустингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}\n",
    ".tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-gyq4{background-color:#88CDB2;border-color:inherit;color:#062425;font-weight:bold;text-align:center;vertical-align:middle}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-n863{background-color:#96fffb;text-align:left;vertical-align:middle}\n",
    ".tg .tg-r6x4{background-color:#ecf4ff;text-align:left;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-gyq4\"></th>\n",
    "    <th class=\"tg-amwm\">Бэггинг</th>\n",
    "    <th class=\"tg-amwm\">Бустинг</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Ансамбль</td>\n",
    "    <td class=\"tg-n863\">Использует ансамбль независимых моделей, каждая из которых обучается на случайной подвыборке данных.</td>\n",
    "    <td class=\"tg-r6x4\">Использует последовательный ансамбль моделей, каждая из которых учитывает ошибки предыдущей модели.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Алгоритм</td>\n",
    "    <td class=\"tg-n863\">Можно использовать с любым алгоритмом обучения.</td>\n",
    "    <td class=\"tg-r6x4\">Обычно используют с деревьями решений.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Применение</td>\n",
    "    <td class=\"tg-n863\">Можно использовать для уменьшения дисперсии модели.</td>\n",
    "    <td class=\"tg-r6x4\">Можно использовать для уменьшения как дисперсии, так и смещения модели.</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0ffd1;color: black;border: 3px solid black; padding: 15px; margin-right: 500px; width: 80%;\">\n",
    "\n",
    "Преимущества бустинга:\n",
    "\n",
    "-    Использует последовательное обучение. Это позволяет каждой новой модели учитывать ошибки предыдущих, что может привести к более точным прогнозам.\n",
    "-    Может обнаруживать сложные зависимости в данных, тогда как бэггинг склонен к созданию более простых моделей.\n",
    "-    Может решать различные задачи, включая классификацию, регрессию и ранжирование, тогда как бэггинг чаще используют для решения задач классификации.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, бустинг может работать лучше, когда требуется более точная модель, способная обнаруживать сложные зависимости в данных, и когда задачу можно решить последовательно. Однако бэггинг тоже может быть эффективным, особенно если требуется уменьшить дисперсию модели и улучшить её устойчивость к шуму в данных.\n",
    "\n",
    "Две наиболее популярные разновидности бустинга — это AdaBoost и градиентный бустинг (Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные отличия между AdaBoost и градиентным бустингом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}\n",
    ".tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-cly1{text-align:left;vertical-align:middle}\n",
    ".tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-jdhy{background-color:#96fffb;border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-gyq4{background-color:#88CDB2;border-color:inherit;color:#062425;font-weight:bold;text-align:center;vertical-align:middle}\n",
    ".tg .tg-4qoz{background-color:#ecf4ff;border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-n863{background-color:#96fffb;text-align:left;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-gyq4\"></th>\n",
    "    <th class=\"tg-7btt\">AdaBoost</th>\n",
    "    <th class=\"tg-7btt\">Градиентный бустинг</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Обучение</td>\n",
    "    <td class=\"tg-jdhy\">Обучает модель путём последовательного добавления новых моделей,  заново взвешивающих объекты, на которых допущены ошибки на предыдущих  итерациях.</td>\n",
    "    <td class=\"tg-4qoz\">Обучает модель, используя градиентный спуск и минимизируя функцию потерь.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Сложность моделей</td>\n",
    "    <td class=\"tg-jdhy\">Использует простые модели, такие как деревья решений с одним разделением (decision stumps).</td>\n",
    "    <td class=\"tg-4qoz\">Может использовать более сложные модели, например, деревья решений с несколькими уровнями.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Регуляризация</td>\n",
    "    <td class=\"tg-jdhy\">Не поддерживает регуляризацию.</td>\n",
    "    <td class=\"tg-4qoz\">Поддерживает регуляризацию для уменьшения переобучения.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Шум</td>\n",
    "    <td class=\"tg-n863\">Чувствителен к шуму в данных, так как объекты с ошибками  классификации получают более высокие веса, что может привести к  переобучению.</td>\n",
    "    <td class=\"tg-cly1\">Более устойчив к шуму.</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">\n",
    "\n",
    "В целом, градиентный бустинг обычно даёт более высокое качество модели, чем AdaBoost, за счёт использования более сложных моделей и регуляризации. Однако AdaBoost может быть быстрее и более простым в использовании, особенно в задачах с небольшим объёмом данных.\n",
    "\n",
    "Давайте разберём эти модели более подробно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>AdaBoost (adaptive boosting)</b> — это алгоритм машинного обучения, используемый для задач классификации и регрессии. Он был разработан Йоавом Фройндом и Робертом Шапиром в 1996 году.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost последовательного добавляет «слабые» классификаторы в композицию. Каждый классификатор обучается на выборке, на которой предыдущие классификаторы допустили ошибки. При этом объекты, которые были неправильно классифицированы, получают более высокие веса, чтобы следующий классификатор мог сосредоточиться на них и исправить ошибки.\n",
    "\n",
    "Каждый слабый классификатор представляет собой простую модель, такую как дерево решений с одним разделением (decision stump), которая предсказывает значение целевой переменной на основе одного признака. Каждый классификатор получает вес, который зависит от его точности. При этом более точные классификаторы получают больший вес. Соответственно, AdaBoost обучает набор слабых моделей на последовательно изменяющихся весах данных и комбинирует их в сильную модель.\n",
    "\n",
    "Когда все слабые классификаторы обучены, они объединяются в одну композицию с помощью взвешенного голосования, где вес каждого классификатора зависит от его точности. Эта композиция является итоговой моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\"><b>Основное преимущество AdaBoost </b>— его способность уменьшать ошибку на обучающей выборке с каждой новой итерацией, что приводит к более высокой точности на тестовых данных. Однако он может быть чувствителен к шуму в данных и может приводить к переобучению, если слабые классификаторы слишком сложны.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим пример с двумя классами — синими и оранжевыми метками. Для обучения AdaBoost мы будем использовать базовый алгоритм классификации — решающие деревья глубины 1, которые называют пнями решений.\n",
    "\n",
    "Иллюстрация ниже демонстрирует, как каждый новый классификатор (пень решений) настраивает веса объектов в обучающей выборке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первоначально все объекты имеют одинаковый вес. Затем мы обучаем первый классификатор и вычисляем его ошибку. Ошибка равна количеству объектов, которые были неправильно классифицированы, делённому на общее количество объектов. Как только мы вычислили ошибку, мы можем рассчитать вес этого классификатора. Чем меньше ошибка, тем выше вес.\n",
    "\n",
    "Затем мы пересчитываем веса объектов в обучающей выборке: если объект был неправильно классифицирован, то его вес увеличивается; если объект был правильно классифицирован, то его вес уменьшается. Затем мы повторяем процедуру с новым классификатором, вычисляя ошибку, вес и пересчитывая веса объектов. На каждой итерации мы добавляем новый классификатор к существующим и пересчитываем веса объектов.\n",
    "\n",
    "После нескольких итераций AdaBoost начинает фокусироваться на объектах, которые ему трудно классифицировать. Это означает, что такие объекты получают больший вес. В итоге мы получаем ансамбль классификаторов, каждый из которых специализируется на определённой области пространства признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формальное определение AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычисляет вес классификатора:\n",
    "\n",
    "$ \\alpha_m =log \\frac{1−er r_m}{er r_m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Обновляет веса объектов:\n",
    "\n",
    "$ w_{m+1,i} = w_{m,i} exp (\\alpha_m I (y_i \\ne h_m (x_i))) , $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $ i=1, \\ldots ,n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После $ M $ итераций AdaBoost объединяет классификаторы $ h_1 (x) , \\ldots , h_M (x) $ с помощью взвешенного голосования:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H (x) =sign (\\sum_{m=1}^M \\alpha_m h_m (x)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь функция $ sign(.) $ возвращает знак своего аргумента (т. е. +1 или -1), что соответствует бинарной классификации.\n",
    "\n",
    "AdaBoost был одним из популярных алгоритмов бустинга, однако сейчас чаще применяют градиентный бустинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e0fff3; padding: 15px; color: black; width: 80%;\">  <b>Градиентный бустинг</b> — это алгоритм машинного обучения, использующий последовательное обучение слабых моделей (например, деревьев решений), каждая из которых нацелена на исправление ошибок предыдущей модели.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибки модели измеряются при помощи градиента функции потерь, и каждая последующая модель обучается на остатках (разности между предсказаниями модели и правильными ответами) предыдущих моделей. После обучения всех моделей их предсказания комбинируются в итоговый ансамбль с помощью взвешенного голосования или суммирования.\n",
    "\n",
    "Градиентный бустинг — один из наиболее мощных алгоритмов машинного обучения. Он широко используется в различных областях, таких как рекомендательные системы, обработка естественного языка и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формальное определение градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим формальное определение задачи для градиентного бустинга. Пусть у нас есть обучающая выборка $ (x_i , y_i) $ , где $ x_i $— входные данные, а $ y_i $— их соответствующие метки класса или значения целевой переменной.\n",
    "\n",
    "Будем считать, что $ y_i $ — действительные (для задачи регрессии) или целые (для задачи классификации) числа, каждое из которых соответствует классу.\n",
    "\n",
    "В градиентном бустинге используется модель ансамбля деревьев решений, которая представляет собой сумму $ T $ деревьев с параметрами $ \\theta_j $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ F (x) = \\sum_{j=1}^T f_j (x; \\theta_j) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое дерево $ f_j (x; \\theta_j) $ — это функция, которая принимает входные данные $ x $ и возвращает соответствующий вклад в ответ ансамбля. Цель градиентного бустинга — найти параметры $ \\theta_j $ каждого дерева таким образом, чтобы ансамбль $ F(x) $ минимизировал функционал ошибки (усреднённое значение функции потерь для набора данных или выборки) $ L(y,F(x)) $ на обучающей выборке. Для этого на каждом шаге добавляется новое дерево, которое минимизирует остаточную ошибку, вычисленную как отрицательный градиент функционала ошибки по отношению к предсказаниям текущей модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ r_{ij} =− \\frac{\\partial L (y_i ,F (x_i))}{\\partial F (x_i)}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f_j = {argmin}_f \\sum_{i=1}^n {(f (x_i ; \\theta_j) − r_{ij})}^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь $ r_{ij} $ — это остаточное значение для $i$-го объекта на $j$-м шаге, а $f_i$— новое дерево решений, которое добавляется к ансамблю на $j$-м шаге.\n",
    "\n",
    "Алгоритм продолжает добавлять новые деревья, пока не будет достигнуто заданное количество итераций или пока ошибка на валидационной выборке не перестанет уменьшаться.\n",
    "\n",
    "Градиентный бустинг получил такое название, потому что использует градиент функционала ошибки по предсказаниям модели для построения последовательности деревьев, которые пытаются уменьшить эту ошибку.\n",
    "\n",
    "При построении модели мы обычно оптимизируем функционал ошибки, который зависит от параметров модели. Для нахождения оптимальных параметров мы должны вычислить градиент этого функционала ошибки по параметрам и выполнить шаг градиентного спуска, чтобы переместиться в направлении уменьшения ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">В градиентном бустинге мы также хотим уменьшить функционал ошибки, но делаем это не напрямую, а путём построения последовательности деревьев, каждое из которых пытается уменьшить остаточную ошибку на обучающей выборке. В результате градиентный бустинг можно рассматривать как метод градиентного спуска в пространстве ответов на каждой итерации.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "Градиентный бустинг для регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, на каждой итерации мы добавляем новое дерево, которое приближает остатки модели на текущей итерации и уменьшает функционал ошибки на обучающей выборке, умножаем его прогноз на скорость обучения (learning rate) и добавляем в общую композицию. Обычно градиентный бустинг продолжается до тех пор, пока не достигнута заданная максимальная глубина деревьев или их заданное количество.\n",
    "\n",
    "После построения модели градиентного бустинга мы можем использовать её для предсказания значений целевой переменной на новых объектах. Для этого на вход модели подаётся объект и вычисляется значение предсказания . Как и в случае обучения, вычисление предсказаний происходит путём суммирования предсказаний всех деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Метод градиентного бустинга позволяет получить высокую точность предсказаний на различных задачах регрессии и классификации. Однако он может быть довольно требователен к вычислительным ресурсам и иметь тенденцию к переобучению на небольших выборках. Для уменьшения риска переобучения можно использовать регуляризацию: например, ограничения на глубину деревьев или использование случайного выбора подмножества объектов и признаков на каждой итерации.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../static/img/module_6_5.png'>\n",
    "<img src='../static/img/module_6_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждой итерации градиентного бустинга строится дерево, которое минимизирует функцию потерь, учитывая значения предсказания на предыдущих итерациях. Далее коэффициенты для каждого дерева находятся решением задачи оптимизации. Итоговая функция предсказания — линейная комбинация всех деревьев с учётом их коэффициентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5f5f5; padding: 15px; color: black; width: 80%;\">Для многоклассовой классификации градиентный бустинг обычно используют в сочетании с методом «один против всех» (one-vs-all), где для каждого класса строится своя модель градиентного бустинга, которая отличает этот класс от всех остальных.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор параметров для градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор оптимальных параметров для модели градиентного бустинга — важный этап, который может существенно повлиять на качество предсказания.\n",
    "\n",
    "<b>Основные параметры, которые нужно настроить:</b>\n",
    "\n",
    "-    <b>Learning rate (шаг обучения)</b> — это коэффициент, на который умножается значение градиента на каждой итерации. Маленькие значения learning rate могут способствовать более точным предсказаниям, но требуют большего числа итераций для обучения. Большие значения learning rate могут способствовать более быстрой сходимости, но приводить к переобучению модели.\n",
    "-    <b>Количество деревьев</b> определяет, сколько деревьев будет использоваться в модели. Большее количество деревьев может улучшить качество предсказания, но привести к переобучению модели.\n",
    "-    <b>Максимальная глубина деревьев</b> определяет, сколько раз дерево будет делить данные. Большая глубина деревьев может привести к переобучению модели, слишком маленькая глубина — к недообучению.\n",
    "-    <b>Минимальное число объектов в листе</b> определяет минимальное количество объектов, которые должны оказаться в листе дерева. Большее значение этого параметра может привести к сокращению переобучения модели.\n",
    "-    <b>Размер выборки для построения деревьев</b> определяет, какое количество объектов будет выбрано для построения каждого дерева. Это может способствовать уменьшению шума и улучшению качества предсказания.\n",
    "\n",
    "Чтобы подобрать оптимальные параметры для модели градиентного бустинга, можно использовать перекрёстную проверку (cross-validation), которая позволяет оценить качество модели на независимых выборках данных. Также можно использовать методы оптимизации, такие как Grid Search или Random Search, чтобы автоматически подобрать оптимальные значения параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация для градиентного бустинга\n",
    "\n",
    "<b>Основные методы регуляризации для градиентного бустинга:</b>\n",
    "\n",
    "-    Ограничение глубины деревьев (max_depth) — установка максимальной глубины деревьев может предотвратить переобучение и ускорить обучение.\n",
    "-    Ограничение на число листьев (max_leaf_nodes) — установка максимального количества листьев может предотвратить переобучение и упростить модель.\n",
    "-    Ограничение на минимальное количество выборок в листе (min_samples_leaf) — установка минимального количества выборок в листе может предотвратить переобучение, особенно если выборка маленькая.\n",
    "-    Ограничение на минимальное количество выборок в узле (min_samples_split) — установка минимального количества выборок в узле может предотвратить создание слишком мелких узлов и, как следствие, переобучение.\n",
    "-    Ограничение на максимальное количество признаков при поиске наилучшего разделения (max_features) — установка максимального количества признаков при поиске наилучшего разделения может предотвратить переобучение и ускорить обучение.\n",
    "-    L1-регуляризация (alpha) добавляет штраф к абсолютному значению весов признаков. Это заставляет модель делать выбор между использованием всех признаков с низкой точностью или только нескольких признаков с высокой точностью.\n",
    "-    L2-регуляризация (lambda) добавляет штраф к квадрату весов признаков. Это заставляет модель предпочитать использование всех признаков с умеренной точностью, а не только нескольких признаков с высокой точностью.\n",
    "\n",
    "Комбинация различных методов регуляризации помогает создать более устойчивую модель, уменьшить переобучение и улучшить её обобщающую способность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модификации градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество модификаций градиентного бустинга, которые могут улучшить его производительность и точность.\n",
    "\n",
    "-    <b>XGBoost </b>— ещё один быстрый и эффективный алгоритм градиентного бустинга, который использует решающие деревья и оптимизирует функцию потерь, добавляя регуляризацию и градиент второго порядка.\n",
    "-    <b>LightGBM</b> — быстрый и эффективный алгоритм градиентного бустинга, который использует специальные техники, такие как гистограммы признаков, для ускорения процесса обучения.\n",
    "-    <b>CatBoost </b>— алгоритм градиентного бустинга, который использует категориальные признаки, включая их в процесс обучения без необходимости предварительной обработки данных.\n",
    "\n",
    "Сравнение основных свойств XGBoost, LightGBM и CatBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}\n",
    ".tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-cly1{text-align:left;vertical-align:middle}\n",
    ".tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-tdvk{background-color:#ffffc7;text-align:left;vertical-align:middle}\n",
    ".tg .tg-jdhy{background-color:#96fffb;border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-4qoz{background-color:#ecf4ff;border-color:inherit;text-align:left;vertical-align:middle}\n",
    ".tg .tg-tcq4{background-color:#88CDB2;border-color:inherit;color:#062425;font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-n863{background-color:#96fffb;text-align:left;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-tcq4\">Свойство</th>\n",
    "    <th class=\"tg-7btt\">XGBoost</th>\n",
    "    <th class=\"tg-amwm\">LightGBM</th>\n",
    "    <th class=\"tg-7btt\">CatBoost</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Алгоритм</td>\n",
    "    <td class=\"tg-jdhy\">Градиентный <br>бустинг </td>\n",
    "    <td class=\"tg-tdvk\">Градиентный <br>бустинг </td>\n",
    "    <td class=\"tg-4qoz\">Градиентный <br>бустинг </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Поддержка GPU</td>\n",
    "    <td class=\"tg-jdhy\">Да</td>\n",
    "    <td class=\"tg-tdvk\">Да</td>\n",
    "    <td class=\"tg-4qoz\">Да</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-lboi\">Распределённое обучение</td>\n",
    "    <td class=\"tg-jdhy\">Да</td>\n",
    "    <td class=\"tg-tdvk\">Да</td>\n",
    "    <td class=\"tg-4qoz\">Да</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Поддержка категориальных признаков</td>\n",
    "    <td class=\"tg-n863\">Нет</td>\n",
    "    <td class=\"tg-tdvk\">Да</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Поддержка пропущенных <br>значений </td>\n",
    "    <td class=\"tg-n863\">Да</td>\n",
    "    <td class=\"tg-tdvk\">Да</td>\n",
    "    <td class=\"tg-cly1\">Да</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Скорость обучения</td>\n",
    "    <td class=\"tg-n863\">Средняя</td>\n",
    "    <td class=\"tg-tdvk\">Высокая</td>\n",
    "    <td class=\"tg-cly1\">Средняя</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Размер модели</td>\n",
    "    <td class=\"tg-n863\">Средний</td>\n",
    "    <td class=\"tg-tdvk\">Маленький</td>\n",
    "    <td class=\"tg-cly1\">Средний</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-cly1\">Качество предсказаний</td>\n",
    "    <td class=\"tg-n863\">Высокое</td>\n",
    "    <td class=\"tg-tdvk\">Высокое</td>\n",
    "    <td class=\"tg-cly1\">Высокое</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-    Все три библиотеки реализуют градиентный бустинг.\n",
    "-    Во всех трёх библиотеках есть поддержка GPU, распределённое обучение и поддержка пропущенных значений.\n",
    "-    LightGBM и CatBoost, в отличие от XGBoost, поддерживают категориальные признаки.\n",
    "-    Скорость обучения в LightGBM наиболее высокая, в XGBoost и CatBoost — средняя.\n",
    "-    Размер модели в LightGBM наименьший, в XGBoost и CatBoost — средний.\n",
    "-    Во всех трёх библиотеках качество предсказаний очень высокое, но особенно выделяются LightGBM и CatBoost.\n",
    "\n",
    "Более подробно о XGBoost, LightGBM и CatBoost мы поговорим далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 1px solid white; padding: 5px; margin-right: auto;  width: 80%;\">\n",
    "Давайте резюмируем основные идеи этого юнита.\n",
    "\n",
    "<br><br>\n",
    "<b>Бустинг —</b> это метод машинного обучения, который используют для улучшения качества прогнозов, создаваемых слабыми моделями машинного обучения. При бустинге каждая следующая модель (называемая базовой моделью) фокусируется на тех примерах данных, которые были неправильно классифицированы предыдущими моделями. Каждая базовая модель «улучшает» работу предыдущих моделей. В итоге бустинг представляет собой комбинацию всех базовых моделей, которые работают вместе, чтобы дать лучший результат, чем любая из них может дать по отдельности.\n",
    "\n",
    "Существует несколько подходов для реализации бустинга:\n",
    "\n",
    "1.       AdaBoost.\n",
    "2.       Градиентный бустинг.\n",
    "\n",
    "<b>AdaBoost</b> — это алгоритм бустинга, который используют для улучшения качества классификации или регрессии в машинном обучении. Он создаёт последовательность слабых моделей машинного обучения и комбинирует их в единую сильную модель.\n",
    "\n",
    "На каждой итерации AdaBoost выбирает новую слабую модель, которая фокусируется на тех примерах данных, которые были классифицированы неправильно на предыдущих итерациях. Веса этих неправильно классифицированных примеров увеличиваются, чтобы следующая слабая модель больше сфокусировалась на них. Затем AdaBoost комбинирует все слабые модели, чтобы получить сильную модель, которая даёт более точные прогнозы.\n",
    "\n",
    "<b>Градиентный бустинг</b> — это метод машинного обучения, который используют для создания сильной модели путём последовательного добавления слабых моделей, которые минимизируют ошибку предыдущих моделей.\n",
    "\n",
    "В градиентном бустинге на каждой итерации создаётся новая слабая модель машинного обучения, которая пытается исправить ошибки предыдущей модели. Для этого градиентный бустинг использует градиент функции потерь: он позволяет определить, какие примеры данных предыдущая модель классифицировала неправильно и насколько сильно эти примеры влияют на функцию потерь.\n",
    "\n",
    "Затем градиентный бустинг обучает новую слабую модель на этих неправильно классифицированных примерах и добавляет её в сильную модель в виде слагаемого. Повторяя этот процесс много раз, градиентный бустинг создаёт последовательность слабых моделей, которые совместно дают более точные прогнозы.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
