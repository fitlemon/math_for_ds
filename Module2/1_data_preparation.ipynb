{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "Процесс разработки проекта машинного обучения обычно состоит из следующих шагов:\n",
    "- **Этап 1: определение задачи**\n",
    "- **Этап 2: подготовка данных**\n",
    "- **Этап 3: разработка модели МО**\n",
    "- **Этап 4: запуск модели**\n",
    "\n",
    "На этом занятии мы рассмотрим подробно этап 2, подготовку данных, и ответим на вопрос, как подготовить данные для их использования в проекте машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных включает в себя следующие шаги:\n",
    "- **Сбор данных**: следующий шаг - это сбор данных, необходимых для обучения модели. Можно собрать данные самостоятельно или использовать уже существующие наборы данных.\n",
    "- **Исследование данных (англ. Exploratory Data Analysis, EDA)**: это процесс исследования и анализа данных, используемый для получения полезной информации и выявления особенностей данных перед применением модели машинного обучения. EDA включает в себя методы визуализации и статистические техники, которые помогают понять данные и выделить важные характеристики.\n",
    "- **Очистка и исправление данных**: очистка данных от пропусков, дубликатов и ошибок.\n",
    "- **Разделение выборки**: разделение выборки на тренировочную и тестовую является важным шагом в машинном обучении, который помогает оценить качество модели на новых данных. Тренировочная выборка используется для обучения модели, тестовая выборка - для проверки ее качества.\n",
    "- **Предобработка признаков**: преобразование признаков, нормализация признаков и т.д.\n",
    "- **Отбор и создание новых признаков**: отбор и создание новых признаков являются важными этапами предобработки данных в машинном обучении. Хорошо подобранные и созданные признаки могут значительно повысить точность модели. Отбор признаков может быть выполнен двумя способами: автоматически и вручную. Автоматический отбор признаков включает использование алгоритмов, которые оценивают важность каждого признака и выбирают только наиболее значимые. Вручную выбранные признаки обычно определяются на основе экспертного знания в предметной области.\n",
    "\n",
    "Мы будем пользоваться готовыми наборами данных, соответственно, шаг сбора данных не будет рассмотрен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Используемые библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn (также известная как sklearn)** - это библиотека машинного обучения на языке Python, которая предоставляет широкий спектр инструментов для решения задач классификации, регрессии, кластеризации, снижения размерности, выбора моделей и многих других. Библиотека основана на библиотеках NumPy, SciPy и matplotlib и имеет простой и интуитивно понятный интерфейс, что делает ее доступной для использования как для начинающих, так и для продвинутых пользователей.\n",
    "\n",
    "**Pandas** - это библиотека для языка Python, которая предоставляет инструменты для работы с данными и их анализа.\n",
    "\n",
    "Основными структурами данных, которые предоставляет библиотека Pandas, являются:\n",
    "\n",
    "- **Series** - это одномерный массив данных, который можно индексировать. Он может содержать данные любого типа, включая числа, строки, объекты Python и т.д.\n",
    "\n",
    "- **DataFrame** - это двумерный массив данных, представляющий собой таблицу, которую можно рассматривать как набор Series, соединенных друг с другом.\n",
    "\n",
    "Библиотека Pandas предоставляет множество инструментов для работы с данными, включая:\n",
    "\n",
    "- Чтение и запись данных в различных форматах (например, CSV, Excel, SQL).\n",
    "\n",
    "- Извлечение, выборка и фильтрация данных из DataFrame.\n",
    "\n",
    "- Объединение и преобразование данных в DataFrame.\n",
    "\n",
    "- Группировка и агрегирование данных.\n",
    "\n",
    "- Визуализация данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "R5VGCAYhEn6q"
   },
   "outputs": [],
   "source": [
    "#Импорт библиотек и необходимых функций\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYQscMsVth4m"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # отключаем предупреждения о присваивании копии DataFrame\n",
    "pd.options.display.max_columns = None  # отображаем все столбцы\n",
    "pd.options.display.max_rows = None  # отображаем все строки\n",
    "pd.options.display.float_format = '{:.2f}'.format  # форматируем вывод чисел с плавающей точкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYCD1yGi6q2M"
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv('datasets/lecture_02_code_labs_01_pandas.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LXSKDSTq4Fb"
   },
   "outputs": [],
   "source": [
    "# Проверка типа данных\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h93nb1trtwhu"
   },
   "outputs": [],
   "source": [
    "# Просмотр первых 5 строк таблицы\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSOhN05qEn6s"
   },
   "outputs": [],
   "source": [
    "# Просмотр крайних строк таблицы\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WvtUDJzEn6t"
   },
   "outputs": [],
   "source": [
    "# Информация о таблице\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80OvPRhPp85x"
   },
   "outputs": [],
   "source": [
    "# Первичное исследование данных можно провести с помощью стандартных средств pandas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для EDA также можно использовать сторонние библиотеки, такие как ydata-profiling\n",
    "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "# profile.to_notebook_iframe()\n",
    "# profile.to_file(\"lecture_02_code_labs_01_pandas.html\") # сохраниние отчета в файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Очистка и исправление данных\n",
    "Очистка данных - это процесс предварительной обработки данных, направленный на удаление или исправление ошибочных, неактуальных, неполных, поврежденных, дублированных или неправильно форматированных данных, чтобы улучшить качество и точность модели машинного обучения.\n",
    "Ниже перечислены некоторые методы очистки данных в машинном обучении:\n",
    "- **Удаление дубликатов**\n",
    "Дубликаты могут привести к неверным результатам, поэтому необходимо удалить все дубликаты в данных.\n",
    "- **Исправление ошибок:** В данных могут быть ошибки, например, опечатки или неверные значения. Такие ошибки могут быть исправлены с помощью автоматических методов, таких как исправление опечаток.\n",
    "Эти методы могут быть применены как отдельно, так и в сочетании друг с другом. Очистка данных является важным шагом в процессе подготовки данных для модели машинного обучения и может существенно улучшить качество модели.\n",
    "\n",
    "В данном датасете нет текстовых данных, поэтому исправление ошибок применяться не будет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаление дубликатов,\n",
    "print(df.shape)\n",
    "# используется метод drop_duplicates() с параметром inplace=True для удаления дубликатов в исходном DataFrame. \n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape) # удалено 16 дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделение выборки на тренировочную и тестовую\n",
    "Scikit-learn содержит множество алгоритмов, включая методы разбиения данных. Scikit-learn (sklearn) предоставляет несколько методов разбиения выборки на тренировочную и тестовую части:\n",
    "\n",
    "**train_test_split** - случайное разбиение выборки на тренировочную и тестовую части. Этот метод наиболее часто используется для оценки качества модели.\n",
    "\n",
    "**KFold** - K-fold кросс-валидация, метод позволяющий оценить качество модели на нескольких подмножествах данных. Выборка разбивается на K частей, каждая из которых используется в качестве тестовой выборки для одного из K экспериментов, а остальные K-1 частей используются для обучения модели.\n",
    "\n",
    "**StratifiedKFold** - вариант K-fold кросс-валидации с сохранением соотношения классов в каждой из частей. Это особенно важно, когда имеются несбалансированные классы.\n",
    "\n",
    "**LeaveOneOut** - метод, при котором каждый объект выборки последовательно используется в качестве тестового, а остальные - для обучения.\n",
    "\n",
    "**ShuffleSplit** - случайный перетасованный разброс, выборка разбивается на n_iter наборов из train_size тренировочных и test_size тестовых объектов. Каждый разброс независим от других.\n",
    "\n",
    "**StratifiedShuffleSplit** - вариант ShuffleSplit, сохраняющий соотношение классов в каждом из наборов.\n",
    "\n",
    "**TimeSeriesSplit** - разбиение временных рядов на тренировочную и тестовую выборки, учитывающее хронологический порядок данных.\n",
    "\n",
    "Для использования любого из этих методов в библиотеке sklearn необходимо импортировать соответствующий класс. Например, для использования метода train_test_split необходимо выполнить следующий код:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "```\n",
    "где X и y - данные и целевая переменная, test_size - доля выборки, которую необходимо выделить в качестве тестовой, а random_state - начальное значение для генератора псевдослучайных чисел.\n",
    "\n",
    "В данном практическом занятии мы воспользуемся методом `train_test_split`, в занятии с KNN мы рассмотрим метод кросс-валидации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделяем данные на обучающую и тестовую выборки\n",
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,  # доля тестовой выборки\n",
    "    random_state=1  # для воспроизводимости результата\n",
    ")\n",
    "\n",
    "# выводим размерности полученных выборок\n",
    "print('Размерность тренировочной выборки:', train.shape)\n",
    "print('Размерность тестовой выборки:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Виды признаков\n",
    "В машинном обучении существуют различные виды признаков, которые могут использоваться для описания объектов или данных. Некоторые из них включают:\n",
    "- **Категориальные признаки**: признаки, которые принимают значения из определенного набора категорий или классов. Примерами категориальных признаков могут служить цвет, тип материала и т.д.\n",
    "- **Числовые признаки**: признаки, которые принимают числовые значения, такие как длина, ширина, высота, возраст и т.д.\n",
    "- **Бинарные признаки**: признаки, которые могут принимать только два значения, например, 0 и 1, да или нет, и т.д.\n",
    "- **Текстовые признаки**: признаки, которые описывают текстовые данные, такие как заголовки новостей, описания продуктов и т.д.\n",
    "- **Географические признаки**: признаки, которые описывают географические данные, такие как координаты, адреса и т.д.\n",
    "- **Временные признаки**: признаки, которые описывают данные, относящиеся ко времени — дата, время, длительность и т.д.\n",
    "\n",
    "Работа с разными видами признаков в машинном обучении может иметь свои особенности, так как каждый вид признаков имеет свои уникальные свойства и может требовать специфических методов работы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98WJCDMPEn6t"
   },
   "source": [
    "# Предобработка признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPjmx9lAf8jb",
    "tags": []
   },
   "source": [
    "## Обработка выбросов\n",
    "В данной выборке мы не будем удалять выбросы, так как даннные собраны из разных стран, и разброс в значениях может быть обусловлен реальными факторами.\n",
    "Приведем здесь основные примеры определения и удаления выбросов:\n",
    "- Метод межквартильного размаха (interquartile range - IQR): Этот метод основан на вычислении межквартильного размаха данных, который определяет расстояние между 25-м и 75-м процентилем данных. Затем выбросы определяются как значения, находящиеся за пределами верхнего и нижнего порогов, определяемых как Q1 - 1.5 * IQR и Q3 + 1.5 * IQR соответственно.\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3, 4, 5, 100],\n",
    "    'col2': [10, 20, 30, 40, 50, 1000] # создаем DataFrame\n",
    "})\n",
    "Q1 = df['col1'].quantile(0.25) # определяем квартили для столбца 'col1'\n",
    "Q3 = df['col1'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['col1'] >= Q1 - 1.5*IQR) & (df['col1'] <= Q3 + 1.5*IQR)] # удаляем выбросы в столбце 'col1'\n",
    "```\n",
    "\n",
    "- Удаление выбросов на основе статистических критериев: Этот метод использует статистические критерии, такие как Z-оценка или T-тест, для определения, является ли значение выбросом или нет. Если значение превышает определенный пороговый уровень, оно считается выбросом и удаляется.\n",
    "```python\n",
    "import scipy\n",
    "df = pd.DataFrame({\n",
    "    'col1': [1, 2, 3, 4, 5, 100],\n",
    "    'col2': [10, 20, 30, 40, 50, 1000] # создаем DataFrame\n",
    "})\n",
    "df = df[(np.abs(scipy.stats.zscore(df['col1'])) < 3)] # удаляем выбросы в столбце 'col1' на основе Z-оценки\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполнение пропущенных значений\n",
    "Некоторые значения могут быть пропущены или недоступны, например, из-за ошибок или недоступности источника данных. Пропущенные значения можно заменить средним, медианным или модальным значением, или использовать другие методы заполнения, например, на основе предсказаний модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общее количество отсутствующих элементов по столбцам\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fi0jPWSozbTY"
   },
   "outputs": [],
   "source": [
    "# df.dropna(how = 'any', inplace = True)\n",
    "# удаляем строки, в которых есть не менее 5 пропущенных значения\n",
    "# train.dropna(thresh=5, inplace=True)\n",
    "# test.dropna(thresh=5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализация пропущенных значений\n",
    "sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# визуализация пропущенных значений\n",
    "sns.heatmap(test.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# заполняем пропущенные значения средним значением по столбцу\n",
    "mean_values = train.mean(numeric_only=True)\n",
    "train.fillna(value=mean_values, inplace=True);\n",
    "test.fillna(value=mean_values, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# визуализация пропущенных значений\n",
    "sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw8fb0YQ29GT"
   },
   "source": [
    "## Преобразование данных в числовой формат\n",
    "Если вы имеете дело с данными в Pandas DataFrame, которые находятся в текстовом или строковом формате, вы можете использовать функции преобразования типов данных для преобразования их в числовой формат.\n",
    "\n",
    "- astype(): Метод astype() может быть использован для преобразования столбца в числовой формат. Пример использования:\n",
    "```python\n",
    "df = pd.DataFrame({'A': ['1', '2', '3'], 'B': ['4', '5', '6']}) # Создаем DataFrame\n",
    "df['A'] = df['A'].astype(int) # Преобразование столбца 'A' в числовой формат\n",
    "```\n",
    "- to_numeric(): Функция to_numeric() может быть использована для преобразования столбца или Series в числовой формат. Эта функция также может обрабатывать значения, которые не могут быть преобразованы в числовой формат, и заменять их на NaN. Пример использования:\n",
    "```python\n",
    "df = pd.DataFrame({'A': ['1', '2', '3'], 'B': ['4', '5', '6']}) # Создаем DataFrame\n",
    "df['A'] = pd.to_numeric(df['A'], errors='coerce') # Преобразование столбца 'A' в числовой формат\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvyZ_OY8kOfi"
   },
   "source": [
    "## Нормализация и масштабирование признаков\n",
    "\n",
    "Нормализация данных - это важный процесс подготовки данных для многих алгоритмов машинного обучения. Ее целью является приведение значений признаков к одному и тому же диапазону, что может улучшить производительность моделей и снизить возможные проблемы с выбросами.\n",
    "\n",
    "Вот несколько причин, по которым может понадобиться применять нормализацию данных:\n",
    "\n",
    "- Разные шкалы измерения: Если в ваших данных есть признаки с разными единицами измерения (например, возраст в годах и доход в долларах), то нормализация может быть полезной для приведения их к одному и тому же масштабу.\n",
    "\n",
    "- Алгоритмы, зависящие от расстояний: Многие алгоритмы машинного обучения (например, метод k-средних или метод ближайших соседей) используют расстояния между объектами для принятия решений. Если признаки не нормализованы, то признаки с большими значениями могут оказать большое влияние на расстояние и перекосить результаты.\n",
    "\n",
    "- Статистические методы: Нормализация может быть полезной для статистических методов, таких как линейная регрессия или анализ главных компонент, которые используют стандартные отклонения для вычисления весов признаков.\n",
    "\n",
    "- Стабильность обучения: Нормализация может помочь уменьшить влияние выбросов в данных, что может улучшить стабильность и производительность модели.\n",
    "\n",
    "В целом, если вы работаете с данными, которые содержат признаки с разными единицами измерения или используете алгоритмы, которые зависят от расстояний между объектами, то нормализация может быть полезной для улучшения производительности вашей модели.\n",
    "\n",
    "Нормализация обычно используется, когда данные имеют распределение, которое сильно отклоняется от нормального распределения. Например, когда данные имеют сильные выбросы или содержат несколько групп, каждая из которых имеет свой диапазон значений. Нормализация помогает сжать диапазон значений признаков в заданный диапазон (например, [0,1]) и обеспечить более равномерное распределение значений.\n",
    "\n",
    "Стандартизация обычно используется, когда данные имеют распределение, близкое к нормальному, и когда важно сохранить среднее значение и стандартное отклонение признаков. Стандартизация позволяет преобразовать данные таким образом, чтобы они имели среднее значение 0 и стандартное отклонение 1. Это может быть полезно для алгоритмов, которые используют стандартные отклонения или коэффициенты корреляции признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализация (MinMax нормализация):** Этот метод преобразует значения признаков в диапазон от 0 до 1 или от -1 до 1. Это можно сделать с помощью формулы: X_norm = (X - X_min) / (X_max - X_min), где X - значение признака, X_min и X_max - минимальное и максимальное значения признака соответственно. В библиотеке pandas для этого можно использовать класс MinMaxScaler из модуля sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train['ИМТ'] = scaler.fit_transform(train['ИМТ'].values.reshape(-1,1))\n",
    "train['ИМТ'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ИМТ'] = scaler.transform(test['ИМТ'].values.reshape(-1,1))\n",
    "test['ИМТ'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стандартизации данных в pandas можно использовать метод scikit-learn StandardScaler. Для этого нужно импортировать класс StandardScaler из модуля sklearn.preprocessing и создать экземпляр этого класса. Затем можно использовать метод fit_transform для обработки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBeh-TlDJ8E8"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train['Общие расходы'] = scaler.fit_transform(train['Общие расходы'].values.reshape(-1,1))\n",
    "round(train['Общие расходы']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Общие расходы'] = scaler.transform(test['Общие расходы'].values.reshape(-1,1))\n",
    "round(test['Общие расходы']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование категориальных признаков\n",
    "\n",
    "One-hot encoding - это процесс преобразования категориальных данных в числовые признаки. При выполнении one-hot encoding необходимо учитывать, что разделение выборки на train и test может привести к проблемам, если в train и test наборах данных содержатся различные уникальные значения категориального признака. Для того, чтобы решить эту проблему, необходимо выполнить one-hot encoding на train и test наборах данных, используя одинаковые наборы столбцов-индикаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# выполняем one-hot encoding на train и test наборах данных\n",
    "# используем параметр columns для указания списка столбцов, которые нужно преобразовать\n",
    "train = pd.get_dummies(train, columns=['Статус'])\n",
    "test = pd.get_dummies(test, columns=['Статус'])\n",
    "\n",
    "# выравниваем наборы столбцов-индикаторов в train и test DataFrame,\n",
    "# чтобы обеспечить одинаковый набор столбцов в обоих наборах данных\n",
    "train, test = train.align(test, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "# печатаем результат\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding - это метод кодирования категориальных признаков, при котором каждому уникальному значению признака сопоставляется уникальный целочисленный идентификатор. Этот метод широко используется в машинном обучении для преобразования категориальных признаков в числовые, когда между категориями нельзя установить порядок или иерархию.\n",
    "\n",
    "Например, если у нас есть категориальный признак \"цвет\", который может принимать значения \"красный\", \"зеленый\" и \"синий\", то мы можем использовать label encoding, чтобы преобразовать его в числовой признак\n",
    "\n",
    "При выполнении label encoding необходимо учитывать, что разделение выборки на train и test может привести к проблемам, если в train и test наборах данных содержатся различные уникальные значения категориального признака. Для того, чтобы решить эту проблему, необходимо выполнить label encoding на train и test наборах данных с использованием одного и того же словаря кодирования.\n",
    "\n",
    "В данном датасете нет категориальных признаков без отношения порядка, поэтому приведем пример кода, который выполняет label encoding с учетом разделения выборки на train и test:\n",
    "```python\n",
    "le = LabelEncoder() # создаем экземпляр класса LabelEncoder и обучаем его на train наборе данных на столбце \"color\"\n",
    "le.fit_transform(train['color'])\n",
    "test_df['color_encoded'] = le.transform(test_df['color']) # преобразуем значения столбца 'color' в train и test DataFrame, используя обученный LabelEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean encoding**, также известное как **target encoding**, - это техника предобработки признаков, используемая для преобразования категориальных переменных в числовые. При mean encoding числовое значение, присвоенное каждой категории, основывается на среднем значении целевой переменной для этой категории. Mean encoding может помочь улавливать информацию о взаимосвязи между категориальной переменной и целевой переменной, что может улучшить производительность моделей машинного обучения.\n",
    "\n",
    "Вот пример того, как выполнить mean encoding с помощью Pandas:\n",
    "```python\n",
    "df = pd.DataFrame({'category': ['A', 'B', 'A', 'B', 'C', 'C', 'C', 'B'],\n",
    "                   'target': [1, 0, 1, 0, 1, 0, 1, 0]})\n",
    "mean_target = df.groupby('category')['target'].mean() # Рассчет среднего значения целевой переменной для каждой категории\n",
    "df['category_mean'] = df['category'].map(mean_target) # Отображение среднего значения целевой переменной для каждой категории в dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Encoding - это техника преобразования категориальных признаков в числа, используя частоты (встречаемость) каждой категории в данном признаке.\n",
    "В Pandas можно легко выполнить Frequency Encoding, используя метод value_counts() и метод map() для присвоения частот категориям.\n",
    "\n",
    "Вот пример, который демонстрирует, как выполнить Frequency Encoding в Pandas:\n",
    "```python\n",
    "df = pd.DataFrame({'category': ['A', 'B', 'A', 'B', 'C', 'C', 'C', 'B']}) # Создание образца dataframe\n",
    "freq = df['category'].value_counts(normalize=True) # Получение частот каждой категории в столбце 'category'\n",
    "df['category_freq'] = df['category'].map(freq) # Отображение частот для каждой категории в dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание новых признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инженерия признаков на основе знаний об отрасли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть набор данных, содержащий информацию о клиентах банка, включая их возраст, доход, количество детей и т.д. Одним из возможных способов создания новых признаков на основе знаний об отрасли может быть использование данных о средней зарплате в регионе клиента.\n",
    "\n",
    "Для этого можно загрузить данные о средней зарплате из внешнего источника, например, в CSV-файл, и затем объединить эти данные с исходным набором данных при помощи функции merge() из библиотеки pandas.\n",
    "```python\n",
    "import pandas as pd # Загрузка данных о клиентах банка\n",
    "data = pd.read_csv(\"bank_customers.csv\") # Загрузка данных о средней зарплате в регионах\n",
    "salary_data = pd.read_csv(\"region_salary.csv\") # Объединение данных\n",
    "data = pd.merge(data, salary_data, on=\"region\", how=\"left\") # Создание нового признака \"отношение дохода к средней зарплате в регионе\"\n",
    "data[\"income_to_region_salary_ratio\"] = data[\"income\"] / data[\"region_salary\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование существующих признаков\n",
    "Рассмотрим создание новых признаков, например, на основе логарифмирования данных. Логарифмирование может быть полезным, если значения признаков распределены неравномерно и имеют большой разброс. Логарифмирование может помочь снизить влияние экстремальных значений и сделать распределение более нормальным. Нужно помнить, что нельзя взять логарифм от нуля или отрицательного числа. Такие значения не имеют определенного логарифма, поэтому Python выдает предупреждение и возвращает бесконечность (inf) или NaN (Not a Number). Чтобы исправить эту ошибку, необходимо проверить данные на наличие нулей и отрицательных значений перед логарифмированием.\n",
    "\n",
    "В библиотеке pandas для логарифмирования можно использовать функцию np.log() из библиотеки numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создание нового признака \"логарифм значения\"\n",
    "train[\"log_Корь\"] = np.log(train[\"Корь\"] + 0.1)\n",
    "test[\"log_Корь\"] = np.log(test[\"Корь\"] + 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков\n",
    "Отбор признаков (feature selection) — это процесс выбора наиболее значимых признаков из набора признаков для использования в модели машинного обучения. Это может улучшить производительность модели, уменьшить время обучения и снизить риск переобучения.\n",
    "\n",
    "В Python существует несколько библиотек для отбора признаков, включая:\n",
    "\n",
    "- Scikit-learn: Одна из наиболее популярных библиотек машинного обучения в Python, которая содержит множество методов отбора признаков, включая методы на основе статистики, регрессии, решающих деревьев, а также методы на основе ансамблей.\n",
    "\n",
    "- Boruta: Библиотека, основанная на алгоритме Boruta, который позволяет находить наиболее важные признаки в наборе данных, используя рандомизированные деревья.\n",
    "\n",
    "- Featuretools: Библиотека для автоматического создания новых признаков на основе существующих и отбора наиболее значимых признаков с помощью метода важности признаков.\n",
    "\n",
    "- Yellowbrick: Библиотека для визуализации моделей машинного обучения и выбора наиболее значимых признаков.\n",
    "\n",
    "Пример отбора признаков с использованием метода важности признаков в Scikit-learn:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris() # Загрузка данных\n",
    "X = data.data\n",
    "y = data.target\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42)) # Отбор признаков с помощью RandomForestClassifier\n",
    "selector.fit(X, y)\n",
    "feature_importances = selector.estimator_.feature_importances_ # Вывод наиболее значимых признаков\n",
    "feature_names = data.feature_names\n",
    "for i in selector.get_support(indices=True):\n",
    "    print(feature_names[i], feature_importances[i])\n",
    "```\n",
    "Этот код отбирает наиболее значимые признаки с использованием случайного леса и выводит их вместе с их важностью.\n",
    "Алгоритм случайного леса будет рассмотрен в 5 модуле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Wrangling & EDA with Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
