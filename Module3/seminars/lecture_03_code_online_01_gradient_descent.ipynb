{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n",
    "- [Вектор градиента](#gradient)\n",
    "- [Формула градиента линейной регрессии](#gradient_linear_regression)\n",
    "- [Аналитическое решение](#analitic)\n",
    "- [Отладчик](#debugger)\n",
    "- [Реализация в Python](#linear_regression_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вектор градиента  <a class=\"anchor\" id=\"gradient\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор градиента - это вектор, который указывает направление наибольшего увеличения функции, а его длина определяет скорость роста функции в этом направлении. В машинном обучении вектор градиента используется для оптимизации функции потерь, которая является мерой различия между предсказанными моделью значениями и фактическими значениями.\n",
    "\n",
    "В частности, в градиентном спуске используется вектор градиента для нахождения минимума функции потерь путем итеративного изменения параметров модели в направлении, противоположном вектору градиента. Каждый шаг определяется размером шага (learning rate), который определяет, насколько сильно параметры должны быть изменены в каждой итерации.\n",
    "\n",
    "Обозначение вектора градиента по частным производным выглядит так:\n",
    " \n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\dots, \\frac{\\partial f}{\\partial w_n}\\right)$$\n",
    "\n",
    "где $f$ - функция нескольких переменных, $w_1, w_2, \\dots, w_n$ - ее аргументы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/everest_gradient_red_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/mse_optimized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/regression_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/gradient_orthogonal_to_contour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![картинка](images03/gradient_direction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формула градиента линейной регрессии  <a class=\"anchor\" id=\"gradient_linear_regression\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для градиента линейной регрессии:\n",
    "```python\n",
    "    gradient = (1/n) * X.T.dot(errors)\n",
    "```\n",
    "Давайте посмотрим, как она выводится для MSE.\n",
    "\n",
    "Пусть у нас есть обучающий набор данных с $n$ наблюдениями и $d$ признаками. Каждое наблюдение $i$ имеет $d$ признаков $x_{i1}, x_{i2}, ..., x_{id}$ и соответствующий целевой признак $y_i$. Задача линейной регрессии состоит в том, чтобы найти линейную функцию, которая наилучшим образом соответствует данным. Мы можем записать модель линейной регрессии следующим образом:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x) = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d = \\sum_{j=0}^{d-1} w_j x_j,\n",
    "$$\n",
    "\n",
    "где $\\hat{y}$ - это предсказанный ответ, $w_j$ - это веса (коэффициенты) для каждого признака, а $x_j$ - это значение признака $j$ для данного наблюдения.\n",
    "\n",
    "MSE это среднее значение квадрата разности между предсказанным значением $\\hat{y_i}$ и истинным значением $y_i$ для каждого наблюдения $i$ в обучающем наборе:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2.\n",
    "$$\n",
    "\n",
    "Теперь мы можем найти градиент MSE по весам $w_j$, чтобы обновить их в процессе обучения. Градиент MSE по $w_j$ можно записать следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2.\n",
    "$$\n",
    "\n",
    "Далее, мы можем использовать правило цепочки, чтобы найти этот градиент:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i) \\frac{\\partial \\hat{y_i}}{\\partial w_j}.\n",
    "$$\n",
    "\n",
    "Теперь нам нужно найти производную $\\frac{\\partial \\hat{y_i}}{\\partial w_j}$. Мы можем заметить, что $\\hat{y_i}$ является линейной комбинацией весов $w_j$ и признаков $x_j$, поэтому производная по $w_j$ просто равна соответствующему признаку $x. \n",
    "\n",
    "Если $j=0$, то\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left(\\sum_{k=0}^d w_k x_{ik}\\right) = \\frac{\\partial}{\\partial w_0} (w_0 + w_1 x_{i1} + \\cdots + w_d x_{id}) = 1.\n",
    "$$\n",
    "\n",
    "Если $j \\neq 0$, то\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left(\\sum_{k=0}^d w_k x_{ik}\\right) = \\frac{\\partial}{\\partial w_j} (w_0 x_{i0} + w_1 x_{i1} + \\cdots + w_j x_{ij} + \\cdots + w_d x_{id}) = x_{ij}.\n",
    "$$\n",
    "\n",
    "Таким образом, мы можем записать градиент MSE по $w_j$ следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i) x_{ij}, \\qquad j = 0, 1, \\ldots, d-1.\n",
    "$$\n",
    "\n",
    "Эта формула градиента позволяет обновлять веса $w_j$ в процессе обучения, используя градиентный спуск или его вариации. В частности, мы можем использовать эту формулу для обучения линейной регрессии методом наименьших квадратов или стохастическим градиентным спуском."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Простейший случай: смещение и один признак**\n",
    "\n",
    "Модель: $m(x) = w_0 + w_1 x$\n",
    "\n",
    "Параметры: $w_0 , w_1$\n",
    "\n",
    "Функционал ошибки: $Q(w_0, w_1) = \\frac{1}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i)^2$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) 1$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_1} = \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) x_{i1}$\n",
    "\n",
    "Вектор градиента: $\\nabla Q(w_0, w_1) = (\\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) 1, \\frac{2}{n} \\sum_{i=1}^n (w_1 x_i + w_0 - y_i) x_{i1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общий случай: d признаков**\n",
    "\n",
    "Модель: $m(x) = w_0 + w_1 x_1 + \\dots + w_{d - 1} x_{d - 1}$\n",
    "\n",
    "Параметры: $w_0, w_1, \\dots , w_{d - 1}$\n",
    "\n",
    "Функционал ошибки: $Q(w_0, \\dots, w_{d - 1}) = \\frac{1}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i)^2$\n",
    "\n",
    "Первая компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i) 1$\n",
    "\n",
    "...\n",
    "\n",
    "d компонента вектора градиента: $\\frac{\\partial \\text{Q}}{\\partial w_{d-1}} = \\frac{2}{n} \\sum_{i=1}^n (w_0 \\cdot 1 + w_1 \\cdot 1 x_{i1} + \\dots +  w_{d-1} \\cdot x_{id-1} - y_i) x_{id-1}, \\qquad j = 0, 1, \\ldots, d-1$\n",
    "\n",
    "Вектор градиента: $\\nabla Q(w) = \\frac{2}{n} X^T (X \\cdot w - y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитическое решение линейной регрессии  <a class=\"anchor\" id=\"analitic\"></a>\n",
    "\n",
    "Функция ошибки: $ (x_i \\cdot w - y_i)^2 $\n",
    "\n",
    "Производная функции ошибки (по правилу производной сложной функции): $ 2 \\cdot (x_i \\cdot w - y_i) \\cdot x_i $\n",
    "\n",
    "Вектор градиента функционала ошибки в матричном виде: $\\nabla Q(w) = \\frac{2}{n} X^T (X \\cdot w - y) $\n",
    "\n",
    "Приравниваем к 0:\n",
    "\n",
    "$\\frac{2}{n} X^T (X \\cdot w - y) = 0$\n",
    "\n",
    "Убираем дробь, умножив обе части на $\\frac{n}{2}$:\n",
    "\n",
    "$X^T (X \\cdot w - y) = 0$\n",
    "\n",
    "Раскрываем скобки и перенеся $X^T y$:\n",
    "\n",
    "$X^T X \\cdot w = X^T y$\n",
    "\n",
    "Решаем относительно w:\n",
    "\n",
    "$w = (X^T X)^{-1} X^T y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация в Python <a class=\"anchor\" id=\"linear_regression_python\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с циклами <a class=\"anchor\" id=\"linear_regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    y_pred = bias\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        y_pred += weights[i] * X[i]\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = [0] * n_features\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Инициализация градиентов\n",
    "        dw = [0] * n_features\n",
    "        db = 0\n",
    "        \n",
    "        \"\"\"!!! Обратить внимание на этот цикл - по каждому признаку объекта\n",
    "        происходит изменение соответствующего элемента вектора градиента\n",
    "        \"\"\"\n",
    "        # Проход по каждому объекту \n",
    "        for i in range(n_samples):\n",
    "            # Предсказание\n",
    "            y_pred = predict(X[i], weights, bias)\n",
    "            \n",
    "            # Обновление градиентов\n",
    "            error = y_pred - y[i]\n",
    "            for j in range(n_features):\n",
    "                dw[j] += error * X[i][j]\n",
    "            db += error\n",
    "        \n",
    "        # Обновление весов и смещения\n",
    "        for j in range(n_features):\n",
    "            weights[j] -= (learning_rate * dw[j]) / n_samples\n",
    "        bias -= (learning_rate * db) / n_samples\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [1. 1. 1. 1. 1.]\n",
      "Смещение: 2.1316282072803006e-14\n",
      "Предсказанное значение: 59.999999999999964\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3, 4, 5], [4, 5, 6, 7, 8], [7, 8, 9, 10, 11]])\n",
    "y = np.array([15, 30, 45])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y, num_iterations=100)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12, 13, 14])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с numpy <a class=\"anchor\" id=\"linear_regression_numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def linear_regression_numpy(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Предсказание\n",
    "        y_pred = predict(X, weights, bias)\n",
    "\n",
    "        # Обновление градиентов\n",
    "        dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
    "        db = np.mean(y_pred - y)\n",
    "\n",
    "        # Обновление весов и смещения\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.82636543 0.97575707 1.12514872]\n",
      "Смещение: 0.14939164553893752\n",
      "Предсказанное значение: 32.64815835595235\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression_numpy(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с sklearn <a class=\"anchor\" id=\"linear_regression_sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    weights = model.coef_\n",
    "    bias = model.intercept_\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [1. 1. 1. 1. 1.]\n",
      "Смещение: 2.1316282072803006e-14\n",
      "Предсказанное значение: 44.999999999999986\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3, 4, 5], [4, 5, 6, 7, 8], [7, 8, 9, 10, 11]])\n",
    "y = np.array([15, 30, 45])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([7, 8, 9, 10, 11])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с torch <a class=\"anchor\" id=\"linear_regression_torch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.82301044 0.9752538  1.127497  ]\n",
      "Смещение: 0.15224327\n",
      "Предсказанное значение: 32.6401032358408\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def linear_regression(X, y, learning_rate=0.01, num_iterations=10):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = torch.zeros(n_features, requires_grad=True)\n",
    "    bias = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Forward pass\n",
    "        y_pred = torch.matmul(X, weights) + bias\n",
    "        \n",
    "        # Вычисление loss\n",
    "        loss = torch.mean((y_pred - y) ** 2)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Обновление весов и смещения\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad / n_samples\n",
    "            bias -= learning_rate * bias.grad / n_samples\n",
    "            \n",
    "            # Ручное обнуление градиента\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    \n",
    "    return weights.detach().numpy(), bias.detach().numpy()\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([10, 11, 12])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанное значение: tensor([14174.1133], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Преобразование данных в тензоры PyTorch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Определение модели линейной регрессии\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # Один входной признак и один выходной признак\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Инициализация модели\n",
    "model = LinearRegression()\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Цикл обучения\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y.view(-1, 1))\n",
    "    \n",
    "    # Backward pass и оптимизация\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = torch.tensor(np.array([2000, 4000, 8000]), dtype=torch.float32)\n",
    "y_pred = model(x_test)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия с циклами с L2 регуляризацией <a class=\"anchor\" id=\"linear_regression_l2_regularization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, learning_rate=0.01, regularization=0.01, num_iterations=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        dw = np.zeros(n_features)\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            y_pred = predict(X[i], weights, bias)\n",
    "            \n",
    "            dw += (y_pred - y[i]) * X[i]\n",
    "            db += y_pred - y[i]\n",
    "        \"\"\"!!! Обратить внимание на то, как добавляется регуляризация\"\"\"    \n",
    "        # Пояснение, почему регуляризация добавляется к градиенту\n",
    "        # https://datascience.stackexchange.com/questions/111993/why-would-we-add-regularization-loss-to-the-gradient-itself-in-an-svm\n",
    "        # Берется производная по весам (поэтому weights, а не weights в кквадрате) и коэффициент 1/2 перед производной\n",
    "        dw = (dw / n_samples) + (regularization * weights)\n",
    "        db = db / n_samples\n",
    "        \n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    y_pred = np.dot(X, weights) + bias\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса: [0.99707548 0.99936727 1.00165907]\n",
      "Смещение: 0.0051771587861033095\n",
      "Предсказанное значение: 14004.897763120674\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = np.array([6, 15, 24])\n",
    "\n",
    "# Обучение модели\n",
    "weights, bias = linear_regression(X, y, learning_rate=0.01, regularization=0.01, num_iterations=1000)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Веса:\", weights)\n",
    "print(\"Смещение:\", bias)\n",
    "\n",
    "# Пример предсказания\n",
    "x_test = np.array([2000, 4000, 8000])\n",
    "y_pred = predict(x_test, weights, bias)\n",
    "print(\"Предсказанное значение:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
